<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <meta name="theme-color" content="#ffffff">
    <title>Deep Learning for Emotion Recognition in Cartoons</title>
    <style type="text/css">code{white-space: pre;}</style>
    <link rel="apple-touch-icon" sizes="180x180" href="favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">
    <link rel="manifest" href="favicon/manifest.json">
    <link rel="mask-icon" href="favicon/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="stylesheet" href="style.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript" async></script>
    <!--[if lt IE 9]>
        <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
    <![endif]-->
</head>
<body>
<header>
<figure>
<h6 class="university">UNIVERSITY OF LINCOLN</h6>
</figure>
<h1 class="figure title">Deep Learning for Emotion Recognition in Cartoons</h1>
<figure>
<img title="University of Lincoln Coat of Arms" height="200" width ="200"src="./figures/uol.svg" />
</figure>
<figure>
<p>by</p>
<p>John Wesley Hill</p>
<span><a title="PDF Download" href="https://github.com/hako/dissertation/raw/master/dissertation.pdf">PDF</a> &emsp; <a title="HTML (This Page)" href="https://hako.github.io/dissertation/">HTML</a></span>
</figure>
<figure>
    <p>BSc (Hons) Computer Science</p>
    <p>University of Lincoln</p>
    <p>Lincoln School of Computer Science</p>
</figure>

<figure>
<h3>Project Supervisor: Stefanos Kollias</h3>
<h3>5<sup>th</sup> June 2017</h3>
</figure>

<h2 id="chap:contents">Contents</h2>
</header>
<nav id="TOC">
<ul>
<li><a title="Abstract" href="#chap:abstract">Abstract</a>
<li><a title="Acknowledgements" href="#chap:acknowledgements">Acknowledgements</a>
<li><a title="List of Figures" href="#chap:listoffigures">List of Figures</a>
<li><a title="List of Tables" href="#chap:listoftables">List of Tables</a>
<li><a title="Abbreviations" href="#chap:abbreviations">Abbreviations</a>
<li><a title="Nomenclature" href="#chap:abbreviations">Nomenclature</a>
<li><a title="Introduction" href="#sec:chap:introduction">Introduction</a><ul>
<li><a title="Outline" href="#sec:outline">Outline</a></li>
<li><a title="History of Deep Learning" href="#sec:history-of-deep-learning">History of Deep Learning</a></li>
<li><a title="History of Emotion Recognition" href="#sec:history-of-emotion-recognition">History of Emotion Recognition</a></li>
<li><a title="History of Animated Cartoons" href="#sec:history-of-animated-cartoons">History of Animated Cartoons</a></li>
<li><a title="Aim" href="#sec:aim">Aim</a></li>
<li><a title="Objectives" href="#sec:objectives">Objectives</a></li>
<li><a title="Structure of the rest of the report" href="#sec:structure-of-the-rest-of-the-report">Structure of the rest of the report</a></li>
</ul></li>
<li><a title="Background" href="#sec:chap:background">Background</a><ul>
<li><a title="Related Work" href="#sec:related-work">Related Work</a><ul>
<li><a title="Emotion Recognition" href="#sec:emotion-recognition">Emotion Recognition</a></li>
<li><a title="Animated Cartoons" href="#sec:animated-cartoons">Animated Cartoons</a></li>
</ul></li>
<li><a title="Convolutional Neural Networks" href="#sec:convolutional-neural-networks">Convolutional Neural Networks</a></li>
<li><a title="ImageNet" href="#sec:imagenet">ImageNet</a></li>
<li><a title="Recurrent Neural Networks" href="#sec:recurrent-neural-networks">Recurrent Neural Networks</a></li>
</ul></li>
<li><a title="Methodology" href="#sec:chap:methodology">Methodology</a><ul>
<li><a title="Project Management" href="#sec:project-management">Project Management</a><ul>
<li><a title="Trello" href="#sec:trello">Trello</a></li>
</ul></li>
<li><a title="Software Development" href="#sec:software-development">Software Development</a></li>
<li><a title="Research Methods" href="#sec:research-methods">Research Methods</a></li>
</ul></li>
<li><a title="Implementation" href="#sec:chap:implementation">Implementation</a><ul>
<li><a title="Requirements" href="#sec:requirements">Requirements</a><ul>
<li><a title="Choice of Deep Neural Network" href="#sec:choice-of-deep-neural-network">Choice of Deep Neural Network</a></li>
<li><a title="Choice of Animated Cartoon" href="#sec:choice-of-animated-cartoon">Choice of Animated Cartoon</a></li>
<li><a title="Tom &amp; Jerry" href="#sec:tom-jerry"><em>Tom &amp; Jerry</em></a></li>
<li><a title="Dataset Gatherin" href="#sec:dataset-gathering">Dataset Gathering</a></li>
<li><a title="Face Segmentation" href="#sec:face-segmentation">Face Segmentation</a></li>
<li><a title="Haar-like features" href="#sec:haar-like-features">Haar-like features</a></li>
</ul></li>
<li><a title="Design" href="#sec:design">Design</a><ul>
<li><a title="Choice of emotions" href="#sec:choice-of-emotions">Choice of emotions</a></li>
<li><a title="Design of the artefact" href="#sec:design-of-the-artefact">Design of the artefact</a></li>
<li><a title="Design of the Convolutional Neural Network" href="#sec:design-of-the-convolutional-neural-network">Design of the Convolutional Neural Network</a></li>
</ul></li>
<li><a title="Development" href="#sec:development">Development</a><ul>
<li><a title="Tools" href="#sec:tools">Tools</a></li>
<li><a title="Cartoon Face Segmentation" href="#sec:cartoon-face-segmentation">Cartoon Face Segmentation</a></li>
<li><a title="Tom &amp; Jerry - Image Dataset" href="#sec:tom-jerry-image-dataset"><em>Tom &amp; Jerry</em> Image Dataset</a></li>
<li><a title="Training, Classification and Visualisation" href="#sec:training-classification-and-visualisation">Training, Classification and Visualisation</a></li>
</ul></li>
</ul></li>
<li><a title="Testing &amp; Evaluation" href="#sec:chap:testing_evaluation">Testing &amp; Evaluation</a><ul>
<li><a title="Preparation" href="#sec:preparation">Preparation</a></li>
<li><a title="Optimisation Algorithms" href="#sec:optimisation-algorithms">Optimisation Algorithms</a><ul>
<li><a title="Stochastic Gradient Descent" href="#sec:stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
<li><a title="Adagrad" href="#sec:adagrad">Adagrad</a></li>
<li><a title="Adadelta" href="#sec:adadelta">Adadelta</a></li>
<li><a title="RMSProp" href="#sec:rmsprop">RMSProp</a></li>
<li><a title="Adam" href="#sec:adam">Adam</a></li>
</ul></li>
<li><a title="Results" href="#sec:results">Results</a><ul>
<li><a title="Run 1" href="#sec:run-1">Run 1</a></li>
<li><a title="Run 2" href="#sec:run-2">Run 2</a></li>
<li><a title="Run 3" href="#sec:run-3">Run 3</a></li>
<li><a title="Run 4" href="#sec:run-4">Run 4</a></li>
<li><a title="Run 5" href="#sec:run-5">Run 5</a></li>
</ul></li>
</ul></li>
<li><a title="Reflection" href="#sec:chap:reflection">Reflection</a></li>
<li><a title="Cartoon Face Segmentation" href="#sec:cartoon-face-segmentation-1">Cartoon Face Segmentation</a></li>
<li><a title="List of Episodes in Dataset" href="#sec:list-of-episodes-in-dataset">List of Episodes in Dataset</a></li>
<li><a title="Classification" href="#sec:classification">Classification</a></li>
<li><a title="Emotion Classification Results" href="#sec:emotion-classification-results">Emotion Classification Results</a></li>
<li><a title="References" href="#sec:references">References</a></li>
</ul>
</nav>
<figure>
<h3 class="figure title" id="chap:abstract"><i>Abstract</i></h3>
</figure>
<p><i>Emotion Recognition is a field that computers are getting very good at identifying; whether it’s
through images, video or audio. Emotion Recognition has shown promising improvements when
combined with classifiers and Deep Neural Networks showing a validation rate as high as 59% and
a recognition rate of 56%. The focus of this dissertation will be on facial based emotion recognition.
This consists of detecting facial expressions in images and videos. While the majority of research
uses human faces in an attempt to recognise basic emotions, there has been little research on whether
the same deep learning techniques can be applied to faces in cartoons. The system implemented in
this paper, aims to classify at most three emotions (happiness, anger and surprise) of the 6 basic
emotions proposed by psychologists Ekman and Friesen, with an accuracy of <b>80%</b> for the 3 emotions.
Showing promise of applications of deep learning and cartoons. This project is an attempt to examine
if emotions in cartoons can be detected in the same way that human faces can.</i></p>

<h2 id="chap:acknowledgements">Acknowledgements</h2>

<p>Throughout my time and dedication to finish this dissertation, I would like to thank the following
    people for their support and advice</p>

<li><b>Professor Stefanos Kollias</b>, my supervisor who provided so much support, advice and
resources in my research.</li>
&nbsp;
<li><b>My family and friends</b>, for their support and patience.</li>
&nbsp;
<li><b>The University of Lincoln Library</b>, in being able to provide the book <i>‘Modern Machine
Learning Techniques and Their Applications in Cartoon Animation Research’</i> through their
inter-library loan system, which I could not get a hold of myself.</li>

<h2 id="chap:listoffigures">List of Figures</h2>
<ul>
    <li><a title="An example of the McCulloch-Pitts (MCP) neuron." href="#fig:mcpneuron">An example of the McCulloch-Pitts (MCP) neuron.</a></li>
    <li><a title="An example of the Perceptron." href="#fig:perceptron">An example of the Perceptron.</a></li>
    <li><a title="The XOR problem." href="#fig:xor_problem">The XOR problem.</a></li>
    <li><a title="A Multi-Layer Perceptron" href="#fig:mlp_xor">A Multi-Layer Perceptron</a></li>
    <li><a title="Convolutional Neural Network." href="#fig:convnet">Convolutional Neural Network.</a></li>
    <li><a title="Recurrent Neural Network & Vanishing Gradient Problem." href="#fig:vgp">Recurrent Neural Network & Vanishing Gradient Problem.</a></li>
    <li><a title="LSTM unit." href="#fig:lstm">LSTM unit.</a></li>
    <li><a title="An illustrated example of parameter sharing." href="#fig:shared_weights">An illustrated example of parameter sharing.</a></li>
    <li><a title="An example of a 2D convolution." href="#fig:2d_convolution">An example of a 2D convolution.</a></li>
    <li><a title="Max pooling." href="#fig:maxpool">Max pooling.</a></li>
    <li><a title="Fully Connected Network & ReLU." href="#fig:fully_connected_output_layers_relu">Fully Connected Network & ReLU.</a></li>
    <li><a title="An illustrated example of the softmax function." href="#fig:softmax">An illustrated example of the softmax function.</a></li>
    <li><a title="AlexNet architecture." href="#fig:alexnet">AlexNet architecture.</a></li>
    <li><a title="ImageNet classification on Cartoons." href="#fig:inception">ImageNet classification on Cartoons.</a></li>
    <li><a title="Trello Board." href="#fig:trello"><i>Trello</i> Board.</a></li>
    <li><a title="This project’s Trello board" href="#fig:trello_2">This project’s <i>Trello</i> board.</a></li>
    <li><a title="A scene from Tom & Jerry." href="#fig:tom_and_jerry">A scene from Tom & Jerry.</a></li>
    <li><a title="YouTube results for the query ‘Tom & Jerry’." href="#fig:tom_and_jerry_dataset_gathering_1"><i>YouTube</i> results for the query ‘Tom & Jerry’.</a></li>
    <li><a title="Sample set of videos from the Joni Valentayn YouTube channel." href="#fig:tom_and_jerry_dataset_gathering_2">Sample set of videos from the <i>Joni Valentayn</i> <i>YouTube</i> channel.</a></li>
    <li><a title="Different Haar-like features." href="#fig:haar_like">Different Haar-like features.</a></li>
    <li><a title="Haar-like features detecting features in a face." href="#fig:viola_jones">Haar-like features detecting features in a face.</a></li>
    <li><a title="The architecture of the first step process, dataset construction & segmentation." href="#fig:design_1">The architecture of the 1<sup>st</sup> step process, dataset construction & segmentation.</a></li>
    <li><a title="The architecture of the second step process." href="#fig:design_2">The architecture of the 2<sup>nd</sup> step process.</a></li>
    <li><a title="Haar cascade training for positive images." href="#fig:haar_positive_images">Haar cascade training for positive images.</a></li>
    <li><a title="Before and after segmenting a region of a face" href="#fig:segmentation_1">Before and after segmenting a region of a face</a></li>
    <li><a title="Result after multi-class classification." href="#fig:example_results">Result after multi-class classification.</a></li>
    <li><a title="Convolution visualisations." href="#fig:cnn_visualisation">Convolution visualisations.</a></li>
    <li><a title="Results Graphs, Run 1." href="#fig:results_graphs_1">Results Graphs, Run 1.</a></li>
    <li><a title="Results Graphs, Run 2." href="#fig:results_graphs_2">Results Graphs, Run 2.</a></li>
    <li><a title="Results Graphs, Run 3." href="#fig:results_graphs_3">Results Graphs, Run 3.</a></li>
    <li><a title="Results Graphs, Run 4." href="#fig:results_graphs_4">Results Graphs, Run 4.</a></li>
    <li><a title="Results Graphs, Run 5." href="#fig:results_graphs_5">Results Graphs, Run 5.</a></li>
</ul>

<h2 id="chap:listoftables">List of Tables</h2>
<ul>
    <li><a title="ImageNet classification of the zebra by (Randomlists.com, n.d) in Figure 2.6." href="#tab:zebra_results">ImageNet classification of the zebra by (Randomlists.com, n.d) in Figure 2.6.</a></li>
    <li><a title="Hyperparameter Table, Run 1." href="#tab:parameters_1">Hyperparameter Table, Run 1.</a></li>
    <li><a title="Results Table, Run 1." href="#tab:results_1">Results Table, Run 1.</a></li>
    <li><a title="Hyperparameter Table, Run 2." href="#tab:parameters_2">Hyperparameter Table, Run 2.</a></li>
    <li><a title="Results Table, Run 2." href="#tab:results_2">Results Table, Run 2.</a></li>
    <li><a title="Hyperparameter Table, Run 3." href="#tab:parameters_3">Hyperparameter Table, Run 3.</a></li>
    <li><a title="Results Table, Run 3." href="#tab:results_3">Results Table, Run 3.</a></li>
    <li><a title="Hyperparameter Table, Run 4." href="#tab:parameters_4">Hyperparameter Table, Run 4.</a></li>
    <li><a title="Results Table, Run 4." href="#tab:results_4">Results Table, Run 4.</a></li>
    <li><a title="Hyperparameter Table, Run 5." href="#tab:parameters_5">Hyperparameter Table, Run 5.</a></li>
    <li><a title="Results Table, Run 5." href="#tab:results_5">Results Table, Run 5.</a></li>
    <li><a title="Algorithms with the best loss accuracy out of all 5 runs." href="#tab:5.11">Algorithms with the best loss accuracy out of all 5 runs.</a></li>
    <li><a title="The original risk matrix from project proposal" href="#tab:6.1">The original risk matrix from project proposal</a></li>
    <li><a title="The first 32 Tom & Jerry episodes of the images that are included in the dataset of
    this project." href="#tab:dataset_videos_1">The first 32 Tom & Jerry episodes of the images that are included in the dataset of
    this project.</a></li>
    <li><a title="The last 32 Tom & Jerry episodes of the images that are included in the dataset of
    this project." href="#tab:dataset_videos_2">The last 32 Tom & Jerry episodes of the images that are included in the dataset of
    this project.</a></li>
</ul>


<h2 id="chap:abbreviations">Abbreviations</h2>
<p><span style="display:inline-block"> 
<strong title="Acted Facial Expression in the Wild">AFEW</strong> &emsp; &emsp; &emsp; &emsp; &emsp; <strong>A</strong>cted <strong>F</strong>acial <strong>E</strong>xpression in the <strong>W</strong>ild<br />
<strong title="Artificial Neural Network">ANN</strong> &emsp; &emsp; &emsp; &emsp; &emsp;&emsp;<strong>A</strong>rtificial <strong>N</strong>eural <strong>N</strong>etwork<br />
<strong title="Convolutional Neural Network">CNN</strong> &emsp; &emsp; &emsp; &emsp; &emsp;&emsp;<strong>C</strong>onvolutional <strong>N</strong>eural <strong>N</strong>etwork<br />
<strong title="Deep Belief Network">DBN</strong> &emsp; &emsp; &emsp; &emsp;&emsp; &emsp;<strong>D</strong>eep <strong>B</strong>elief <strong>N</strong>etwork<br />
<strong title="Facial Expression Recognition-2013">FER-2013</strong>&emsp; &emsp; &emsp;&emsp;<strong>F</strong>acial <strong>E</strong>xpression <strong>R</strong>ecognition-<strong>2013</strong><br />
<strong title="Feed Forward Neural Network">FFNN</strong> &emsp;&emsp; &emsp;&emsp;&emsp;&emsp;<strong>F</strong>eed <strong>F</strong>orward <strong>N</strong>eural <strong>N</strong>etwork<br />
<strong title="Hierarchical Data Format 5">HDF5</strong> &emsp;&emsp; &emsp;&emsp;&emsp;&emsp;<strong>H</strong>ierarchical <strong>D</strong>ata <strong>F</strong>ormat <strong>5</strong><br />
<strong title="Human Computer Interface">HCI</strong>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>H</strong>uman <strong>C</strong>omputer <strong>I</strong>nterface<br />
<strong title="ImageNet Large Scale Visual Recognition Challenge">ILSVRC</strong> &emsp;&emsp; &emsp;&emsp;&emsp;<strong>I</strong>mageNet <strong>L</strong>arge <strong>S</strong>cale <strong>V</strong>isual <strong>R</strong>ecognition <strong>C</strong>hallenge<br />
<strong title="Identity Recurrent Neural Network">IRNN</strong> &emsp;&emsp; &emsp;&emsp;&emsp;&emsp; <strong>I</strong>dentity <strong>R</strong>ecurrent <strong>N</strong>eural <strong>N</strong>etwork<br />
<strong title="McCulloch–Pitts Neuron">MCP</strong> &emsp; &emsp; &emsp; &emsp; &emsp;&emsp;<strong>M</strong>c<strong>C</strong>ulloch–<strong>P</strong>itts Neuron<br />
<strong title="Metro-Goldwyn-Mayer">MGM</strong>&emsp;&emsp; &emsp; &emsp; &emsp;&emsp;<strong>M</strong>etro-<strong>G</strong>oldwyn-<strong>M</strong>ayer<br />
<strong title="Multi–Layered Perceptrons">MLP</strong>&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; <strong>M</strong>ulti–<strong>L</strong>ayered <strong>P</strong>erceptrons<br />
<strong title="Natural Language Processing">NLP</strong>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>N</strong>atural <strong>L</strong>anguage <strong>P</strong>rocessing<br />
<strong title="Neural Turing Machine">NTM</strong>&emsp;&emsp; &emsp; &emsp; &emsp; &emsp;<strong>N</strong>eural <strong>T</strong>uring <strong>M</strong>achine<br />
<strong title="Nesterov Accelerated Gradient">NAG</strong>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp;<strong>N</strong>esterov <strong>A</strong>ccelerated <strong>G</strong>radient<br />
<strong title="Open Computer Vision library" >OpenCV</strong>&emsp;&emsp;&emsp; &emsp;&emsp;<strong>O</strong>pen <strong>C</strong>omputer <strong>V</strong>ision library<br />
<strong title="Python Software Foundation" >PSF</strong>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; <strong>P</strong>ython <strong>S</strong>oftware <strong>F</strong>oundation<br />
<strong title="Long Short Term Memory" >LSTM</strong>&emsp;&emsp; &emsp; &emsp;&emsp;&emsp;<strong>L</strong>ong <strong>S</strong>hort <strong>T</strong>erm <strong>M</strong>emory<br />
<strong title="Rectified Linear Unit" >ReLU</strong>&emsp;&emsp; &emsp; &emsp;&emsp;&emsp;<strong>Re</strong>ctified <strong>L</strong>inear <strong>U</strong>nit<br />
<strong title="Recurrent Neural Network" >RNN</strong>&emsp;&emsp; &emsp; &emsp; &emsp; &emsp;<strong>R</strong>ecurrent <strong>N</strong>eural <strong>N</strong>etwork<br />
<strong title="Root Mean Square" >RMS</strong>&emsp;&emsp; &emsp; &emsp; &emsp; &emsp;<strong>R</strong>oot <strong>M</strong>ean <strong>S</strong>quare<br />
<strong title="Static Facial Expression in the Wild" >SFEW</strong> &emsp; &emsp; &emsp; &emsp; &emsp;<strong>S</strong>tatic <strong>F</strong>acial <strong>E</strong>xpression in the <strong>W</strong>ild<br />
<strong title="Software Development Life Cycle" >SDLC</strong>&emsp;&emsp; &emsp; &emsp;&emsp;&emsp;<strong>S</strong>oftware <strong>D</strong>evelopment <strong>L</strong>ife <strong>C</strong>ycle<br />
<strong title="Stochastic Gradient Decent" >SGD</strong>&emsp;&emsp; &emsp; &emsp; &emsp; &emsp;<strong>S</strong>tochastic <strong>G</strong>radient <strong>D</strong>ecent<br />
<strong title="Toronto Face Dataset" >TFD</strong>&emsp;&emsp; &emsp; &emsp; &emsp; &emsp;<strong>T</strong>oronto <strong>F</strong>ace <strong>D</strong>ataset<br />
<strong title="Unmanned Aerial Vehicle" >UAV</strong>&emsp;&emsp;&emsp; &emsp; &emsp; &emsp;<strong>U</strong>nmanned <strong>A</strong>erial <strong>V</strong>ehicle<br />
<strong title="eXtreme Programming" >XP</strong> &emsp; &emsp; &emsp; &emsp; &emsp; &emsp;e<strong>X</strong>treme <strong>P</strong>rogramming<br />
<strong title="eXtensible Markup Language" >XML</strong> &emsp; &emsp; &emsp; &emsp; &emsp; e<strong>X</strong>tensible <strong>M</strong>arkup <strong>L</strong>anguage<br />
</span></p>
<h2 id="chap:nomenclature">Nomenclature</h2>
<p><span> <span class="math inline">\( \epsilon \)</span> &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; A very small number<br />
<span class="math inline">\( *, \otimes, \circledast \)</span> &emsp; &emsp; &emsp; &emsp; &emsp;Convolution<br />
<span class="math inline">\( \odot \)</span>&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; Element-wise matrix-vector multiplication<br />
<span class="math inline">\( \nabla \)</span>&emsp;&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; Gradient<br />
<span class="math inline">\( \eta \)</span>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; Learning rate<br />
<span class="math inline">\( \gamma \)</span>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; Momentum term<br />
<span class="math inline">\( J(\theta) \)</span>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp;Objective function<br />
<span class="math inline">\( \sigma(x) \)</span>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp;Sigmoid function<br />
<span class="math inline">\( \sigma(\mathbf{z}) \)</span>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp;Softmax function<br />
<span class="math inline">\( \{0,1\} \)</span>&emsp; &emsp; &emsp;&emsp;&emsp;&emsp;The set containing 0 and 1<br />
<span class="math inline">\( \mathbb{Z} \)</span>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp;  &emsp;The set of integer numbers<br />
<span class="math inline">\( \mathbb{R} \)</span> &emsp; &emsp; &emsp; &emsp; &emsp; &emsp;&emsp;The set of real numbers<br />
<span class="math inline">\( \theta \)</span>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; &emsp; Threshold value/Parameter<br />
</span></p>
<h1 id="sec:chap:introduction">Introduction</h1>
<h2 id="sec:outline">Outline</h2>
<p>In this chapter, we introduce the project by exploring the related topics of emotion recognition and deep learning. The history of both subjects alongside a section explaining the motivation of this project is presented. The subject of animated cartoons is introduced, including an explanation of its history, previous research, relevance and importance to the context of emotion recognition and deep learning. The chapter closes by discussing the aims and objectives of the project plus a summary of the remaining chapters in this report.</p>
<h2 id="sec:history-of-deep-learning">History of Deep Learning</h2>
<p>The area of Deep Learning traces back to the 1940’s where artificial intelligence research was about to come to fruition. In 1943, neuroscientists Warren McCulloch and Walter Pitts proposed an artificial neuron known as the <strong>McCulloch-–Pitts (MCP) neuron</strong>. This neuron formed the basis of the first mathematical model of an artificial neuron. Its primary function is to have inputs <span class="math inline">\(x_i\)</span> that is multiplied by the weights <span class="math inline">\(w_i\)</span>, and the neurons sum the values <span class="math inline">\(w_ix_i\)</span> to create a weighted sum <span class="math inline">\( s \)</span>. If this weighted sum <span class="math inline">\( s \)</span> is greater than a certain threshold , <span class="math inline">\( \theta \)</span> then the neuron fires, otherwise not. <span class="citation" data-cites="Marsland:2014:MLA:2692349">(Marsland <a href="#ref-Marsland:2014:MLA:2692349">2014</a>, 41)</span>.</p>
<p>The MCP neuron has some properties worth discussing. The inputs <span class="math inline">\(x_i\)</span> are binary (1 and 0) and the weights <span class="math inline">\(w_i\)</span> can be either positive or negative, between (-1 and 1), and the weighted sum formula is expressed mathematically as:</p>
<div>
<a id="eq:1.1"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:1}
    s = \sum_{i = 1}^n w_i x_i\]</span></td>
<td style="text-align: right;"><span class="math display">\[(1.1)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>The MCP neuron’s threshold <span class="math inline">\( \theta \)</span> is one example of an “activation function&quot;, this is responsible for “firing&quot; or activating a neuron. In the case for the MCP neuron, the activation function is a linear step function (or more similarly a Heaviside function) <span class="citation" data-cites="wang:2017">(Wang, Raj, and Xing <a href="#ref-wang:2017">2017</a>, 9)</span> the threshold activation function is mathematically expressed as:</p>
<div>
<a id="eq:1.2"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:2}
{\displaystyle y = f(s)=\left\{{\begin{array}{rcl}1&amp;{\mbox{for}}&amp;s\geq\theta\\0&amp;{\mbox{for}}&amp;s&lt; \theta\end{array}}\right.}\]</span></td>
<td style="text-align: right;"><span class="math display">\[(1.2)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>When applied, the output <span class="math inline">\( y \)</span> is binary (1 or 0) depending on the threshold criteria <span class="math inline">\( \theta \)</span>, the MCP neuron only produces a binary result in response. The MCP neuron can perform <em>any</em> logical function using (AND, OR, NOT) by setting predetermined thresholds and inputs. Figure <a 
href="#fig:mcpneuron">1.1</a> shows an example MCP neuron.</p>
<figure title="An example of the McCulloch-Pitts (MCP) neuron.">
<a id="fig:mcpneuron"></a>
<img src="./figures/figure_1.svg" alt="An example of the McCulloch-–Pitts (MCP) neuron." /><figcaption><b class="captiontitle">Figure 1.1: </b>An example of the McCulloch-–Pitts (MCP) neuron.<span data-label="fig:mcpneuron"></span></figcaption>
</figure>
<p>Although very basic, the MCP neuron was superseded by another model known as the <span class="math inline">\( \textbf{Perceptron} \)</span>.</p>
<p>The perceptron is a linear classifier coined by Frank Rosenblatt in 1958 that is capable of classifying given inputs into two classes respectively. To put forward an example, as spam filter separating emails into <em>“Spam&quot;</em> and <em>“Not Spam&quot;</em> is a clear use case that a perceptron can solve.</p>
<p>On the surface, Rosenblatt’s perceptron shares some similarity with the MCP neuron. <span class="citation" data-cites="Marsland:2014:MLA:2692349">(Marsland <a href="#ref-Marsland:2014:MLA:2692349">2014</a>, 43)</span> puts it as “nothing more than a collection of McCulloch and Pitts neurons together with a set of inputs and some weights to fasten the inputs to the neurons.&quot; However, there are still some differences between the two.</p>
<p>Firstly, the perceptron includes an independent constant weight called the bias, which is set to 1 or -1. The bias acts as an offset which shifts the input space away from the origin. Figure <a href="#fig:perceptron">1.2</a> shows an example of a perceptron with a bias of 1.</p>
<p>The MCP neuron only has binary outputs (1 or 0) and the perceptron outputs negative or positive values (+1 or -1). Interestingly, the most significant feature of the perceptron is its ability to <strong>learn</strong>, the MCP neuron cannot do this as <span class="citation" data-cites="wang:2017">(Wang, Raj, and Xing <a href="#ref-wang:2017">2017</a>, 9)</span> states that “The weights of [the MCP neuron] <span class="math inline">\( w_i \)</span> are fixed, in contrast to the adjustable weights in [the] modern perceptron&quot;. From a biological standpoint, <span class="citation" data-cites="Rojas:1996:NNS:235222">(Rojas <a href="#ref-Rojas:1996:NNS:235222">1996</a>)</span> argues the ineffectiveness of the MCP neuron stating that they are “too similar to conventional logic gates&quot;.</p>
<p>Perceptrons apply “Hebbian Learning&quot; to learn from data. Named after the psychologist Donald Hebb (under “Hebb’s Rule&quot;), Hebb conjured that the link (weights) between two or more neurons strengthens or weakens given its firing activity. More specifically, “...When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both of the cells such that A’s efficiency, as one of the cells firing B is increased&quot; <span class="citation" data-cites="hebb:1949">(Hebb <a href="#ref-hebb:1949">1949</a>, 62)</span>. In short, both <span class="citation" data-cites="Lowel209">(Lowel and Singer <a href="#ref-Lowel209">1992</a>, 211)</span> and <span class="citation" data-cites="schatz1992developing">(Schatz <a href="#ref-schatz1992developing">1992</a>, 21)</span> condense this rule succinctly: “cells that fire together, wire together&quot;. Mathematically, The perceptron model is an adjusted formula from Equation <a href="#eq:1.1">1.1</a>:</p>
<div>
<a id="eq:1.3"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:3}
    s = \sum_{i = 1}^n w_i x_i + b\]</span></td>
<td style="text-align: right;"><span class="math display">\[(1.3)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>Alongside the Hebbian Learning Rule for updating the weights of the perceptron:</p>
<div>
<a id="eq:1.4"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:4}
    \Delta w_{ij} = \eta x_i y_j\]</span></td>
<td style="text-align: right;"><span class="math display">\[(1.4)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>Where <span class="math inline">\( w_{ij} \)</span> represents the weight change, and <span class="math inline">\( \eta \)</span> represents the learning rate, as it is multiplied by the input weights <span class="math inline">\(x_i\)</span> and the output <span class="math inline">\(y_j\)</span>. With this rule in place, the perceptron adjusts its weights based on the output of the network.</p>
<figure title="An example of the Perceptron.">
<a id="fig:perceptron"></a>
<img src="./figures/figure_2.svg" alt="An example of the Perceptron." /><figcaption><b class="captiontitle">Figure 1.2: </b>An example of the Perceptron.<span data-label="fig:perceptron"></span></figcaption>
</figure>
<p>Rosenblatt proposed a convergence theorem which proves that the perceptron will converge towards a solution such that the data will be separated by a finite number of iterations, given that the data is linearly separable. This notion was challenged by <span class="citation" data-cites="minsky69perceptrons">(Minsky and Papert <a href="#ref-minsky69perceptrons">1969</a>)</span> where they discussed the limitation of the perceptrons ability to solve the XOR (Exclusive OR) function and concluded that the XOR function was not linearly separable. <span class="citation" data-cites="Ertel:2011:IAI:1971988">(Ertel <a href="#ref-Ertel:2011:IAI:1971988">2011</a>, 170)</span> explains this issue further, “...the XOR function does not have a straight line of separation. [Clearly,] the XOR function has a more complex structure than the AND function in this regard.&quot; Figure <a href="#fig:xor_problem">1.3</a> graphically shows why the perceptron cannot solve the XOR problem.</p>
<figure title="The XOR problem.">
<a id="fig:xor_problem"></a>
<img src="./figures/figure_3.svg" alt="The XOR problem as depicted by 170(Ertel 2011, 170) and challenged by (Minsky and Papert 1969) Perceptrons can linearly separate the AND function but not XOR. ( \bullet  true, \circ  false) " /><figcaption>The XOR problem as depicted by <span class="citation" data-cites="Ertel:2011:IAI:1971988">(Ertel <a href="#ref-Ertel:2011:IAI:1971988">2011</a>, 170)</span> and challenged by <span class="citation" data-cites="minsky69perceptrons">(Minsky and Papert <a href="#ref-minsky69perceptrons">1969</a>)</span> Perceptrons can linearly separate the AND function but not XOR. (<span class="math inline">\( \bullet \)</span> true, <span class="math inline">\(\circ \)</span> false) <span data-label="fig:xor_problem"></span></figcaption>
</figure>
<p>Since then, the XOR problem in perceptrons caused a major setback for neural network research, nicknamed the “AI Winter&quot;. Only until the introduction of <strong>Multi-Layer Perceptrons</strong> (MLP) shown in Figure <a href="#fig:mlp_xor">1.4</a> and backpropagation that this issue was eventually solved.</p>
<p>Multi-Layer Perceptrons are different to single layer perceptrons as described above; the difference becomes clear with the introduction of the “Hidden Layer&quot;. “These internal layers are called “hidden“ because they only receive internal inputs and produce internal outputs.” <span class="citation" data-cites="Patterson:1998:ANN:521611">(Patterson <a href="#ref-Patterson:1998:ANN:521611">1998</a>, 142)</span> We call this type of network <strong>Feed Forward Neural Networks</strong> (FFNN) because each perceptron is interconnected and feeds information forward to the next layer of perceptrons, and that “There is no connection among perceptrons in the same layer.&quot; <span class="citation" data-cites="Roberts:2017:Online">(E. Roberts <a href="#ref-Roberts:2017:Online">2006</a>)</span>.</p>
<p>The MLP makes use of the backpropagation algorithm, although the algorithm is not exclusive to MLP’s. The algorithm adjusts the weights of the network based on the “errors&quot; of the output layer. “In this way, errors are propagated backwards layer by layer with corrections being made to the corresponding weights in an iterative manner&quot; <span class="citation" data-cites="Patterson:1998:ANN:521611">(Patterson <a href="#ref-Patterson:1998:ANN:521611">1998</a>)</span>. This process is called gradient descent which is key in backpropagation. <span class="citation" data-cites="Ertel:2011:IAI:1971988">Ertel (<a href="#ref-Ertel:2011:IAI:1971988">2011</a>)</span> mentions that the weight update method is derived from the “delta rule&quot; (an alternative to Hebbian Learning) and uses a sigmoid function (see Equation <a href="#eq:1.5">1.5</a>) as the activation function <span class="citation" data-cites="Ertel:2011:IAI:1971988">(<a href="#ref-Ertel:2011:IAI:1971988">2011</a>, 246)</span>. The sigmoid function <span class="math inline">\( \sigma(x) \)</span> outputs a value within the range (0, 1) whereas the linear step function outputs a value in the exact range {0, 1}.</p>
<div>
<a id="eq:1.5"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:5}
    \sigma(x) = \large\frac{1}{1 + e^{-x}}\]</span></td>
<td style="text-align: right;"><span class="math display">\[(1.5)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>The result of these constant weight readjustments is that the total error is reduced to a minimum. <span class="citation" data-cites="Schmidhuber:2014cz">Schmidhuber (<a href="#ref-Schmidhuber:2014cz">2014</a>)</span> suggests that Paul Werbos was the first to apply an efficient backpropagation algorithm to neural networks in 1981 <span class="citation" data-cites="Schmidhuber:2014cz">(<a href="#ref-Schmidhuber:2014cz">2014</a>, 11)</span> <span class="citation" data-cites="Bishop:1995:NNP:525960">(Bishop <a href="#ref-Bishop:1995:NNP:525960">1995</a>, 141)</span> mentions that backpropagation came to prominence in a paper by <span class="citation" data-cites="Rumelhart:1986:LIR:104279.104293">(Rumelhart, Hinton, and Williams <a href="#ref-Rumelhart:1986:LIR:104279.104293">1986</a>, 2)</span> as an answer to the XOR problem, contending that “...a two-layer system would be able to solve the problem.&quot;. To further their case, they argue that placing a single hidden unit changes the similarity structure of the network to allow the XOR function to be learned <span class="citation" data-cites="Rumelhart:1986:LIR:104279.104293">(Rumelhart, Hinton, and Williams <a href="#ref-Rumelhart:1986:LIR:104279.104293">1986</a>, 3)</span> and conclude with the statement that the “...error propagation scheme leads to solutions in virtually every case.&quot;<span class="citation" data-cites="Rumelhart:1986:LIR:104279.104293">(Rumelhart, Hinton, and Williams <a href="#ref-Rumelhart:1986:LIR:104279.104293">1986</a>, 33)</span>. Backpropagation became a common technique in training neural networks and is still being used today.</p>
<figure title="A Multi-Layer Perceptron">
<a id="fig:mlp_xor"></a>
<img src="./figures/figure_4.svg" alt="A Multi-Layer Perceptron with an architecture described by (Rumelhart, Hinton, and Williams 1986) to learn the XOR problem." /><figcaption><b class="captiontitle">Figure 1.4: </b>A Multi-Layer Perceptron with an architecture described by <span class="citation" data-cites="Rumelhart:1986:LIR:104279.104293">(Rumelhart, Hinton, and Williams <a href="#ref-Rumelhart:1986:LIR:104279.104293">1986</a>)</span> to learn the XOR problem.<span data-label="fig:mlp_xor"></span></figcaption>
</figure>
<p>During the 1980’s and 1990’s adding multiple layers to neural networks were showing promising results and led to breakthroughs in deep learning. The <strong>Neocognitron</strong> was one of those promising models proposed by Kunihiko Fukushima in 1980. The Neocognitron has two types of cells originally coined by <span class="citation" data-cites="hubel1962receptive">(Hubel and Wiesel <a href="#ref-hubel1962receptive">1962</a>, 109)</span>, “S-cells&quot; (Simple cells) are used for feature extraction and “C-cells (Complex cells) are used to recognise distinct features of a pattern regardless of distortion. <span class="citation" data-cites="fukushima1980neocognitron">(Fukushima <a href="#ref-fukushima1980neocognitron">1980</a>, 193)</span> confirms this: “The response of the C-cells of the last layer is not affected by the pattern’s position at all.&quot; In short, the deepest layers in the Neocognitron are less sensitive to shift invariance.</p>
<p>The <strong>Convolutional Neural Network</strong> (CNN) was introduced by <span class="citation" data-cites="LeCun:NIPS1989_293">Y. LeCun et al. (<a href="#ref-LeCun:NIPS1989_293">1990</a>)</span> where it was applied on handwritten digits with a 1% error rate. <span class="citation" data-cites="LeCun:NIPS1989_293">(<a href="#ref-LeCun:NIPS1989_293">1990</a>, 11)</span>. The network is known as “LeNet”, one of the first CNN’s. This successful result was due to handling “...a variety of different problems of digits including variances in position and scale, rotation and squeezing of digits, and even different stroke width of the digit.&quot; <span class="citation" data-cites="wang:2017">(Wang, Raj, and Xing <a href="#ref-wang:2017">2017</a>, 39)</span>. These are attributes similar to the Neocognitron. The LeNet advanced further with the introduction of the “LeNet-5&quot; by <span class="citation" data-cites="LeCun:98">LeCun et al. (<a href="#ref-LeCun:98">1998</a>)</span>, being put to use on recognising handwritten digits with a 0.95% error rate (without distortions) and an error rate of 0.8% (with distortions) <span class="citation" data-cites="LeCun:98">(<a href="#ref-LeCun:98">1998</a>, 2288)</span>. Figure <a href="#fig:convnet">1.5</a> shows an example Convolutional Neural Network.</p>
<figure title="Convolutional Neural Network.">
<a id="fig:convnet"></a>
<img src="./figures/figure_5.svg" alt="A Convolutional Neural Network, this architecture is equivalent to the LeNet-5 architecture by (LeCun et al. 1998)." /><figcaption><b class="captiontitle">Figure 1.5: </b>A Convolutional Neural Network, this architecture is equivalent to the LeNet-5 architecture by <span class="citation" data-cites="LeCun:98">(LeCun et al. <a href="#ref-LeCun:98">1998</a>)</span>.<span data-label="fig:convnet"></span></figcaption>
</figure>
<p>A year before the CNN, another neural network, the <strong>Long Short-Term Memory</strong> (LSTM) Neural Network was invented to solve a specific problem. LSTM’s is an evolved version of the <strong>Recurrent Neural Network</strong>; a network which has cycles, giving it the ability to handle sequential data one element at a time. <span class="citation" data-cites="Lipton:2015tj">(Lipton, Berkowitz, and Elkan <a href="#ref-Lipton:2015tj">2015</a>, 2)</span>. <span class="citation" data-cites="Lipton:2015tj">Lipton, Berkowitz, and Elkan (<a href="#ref-Lipton:2015tj">2015</a>)</span> puts forward that RNN’s are trained using <strong>Backpropagation Through Time</strong> (BPTT) and states that all RNN’s apply it <span class="citation" data-cites="Lipton:2015tj">(<a href="#ref-Lipton:2015tj">2015</a>, 11)</span>. However, training RNN’s was a challenge because of the “vanishing/exploding gradient problem&quot;.</p>
<p>This phenomenon occurs when the RNN backpropagates errors across many time steps. As a result, “[the] error signal decreases exponentially within the time steps the BPTT can trace back&quot; <span class="citation" data-cites="wang:2017">(Wang, Raj, and Xing <a href="#ref-wang:2017">2017</a>, 53)</span> Indicating that learning becomes more difficult as the gradients get tiny over time. The reverse, exploding gradients “can make learning unstable&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 282)</span>, Figure <a href="#fig:vgp">1.6</a> shows an example RNN with an example of the vanishing gradient problem.</p>
<figure title="Recurrent Neural Network & Vanishing Gradient Problem.">
<a id="fig:vgp"></a>
<img src="./figures/figure_6.svg" alt="(Left) An example of Recurrent Neural Network, (Right) An illustrated example of the “Vanishing Gradient problem&quot; with an unfolded RNN as described by (Lipton, Berkowitz, and Elkan 2015). The gradients get smaller at each time step." /><figcaption><b class="captiontitle">Figure 1.6: </b>(Left) An example of Recurrent Neural Network, (Right) An illustrated example of the “Vanishing Gradient problem&quot; with an unfolded RNN as described by <span class="citation" data-cites="Lipton:2015tj">(Lipton, Berkowitz, and Elkan <a href="#ref-Lipton:2015tj">2015</a>)</span>. The gradients get smaller at each time step.<span data-label="fig:vgp"></span></figcaption>
</figure>
<p>LSTM’s are designed to address this problem; by introducing a “memory cell&quot; and gated units, enabling the network to remember information when it needs to selectively. The benefit is that previous sequences are remembered for an extended period without degradation, as opposed to the RNN. Figure <a href="#fig:lstm">1.7</a> describes such an LSTM. <span class="citation" data-cites="Hochreiter:1997:LSM:1246443.1246450">Hochreiter and Schmidhuber (<a href="#ref-Hochreiter:1997:LSM:1246443.1246450">1997</a>)</span> have shown that the LSTM can solve problems after 10 and 1000 time lags, in addition to outperforming other algorithms <span class="citation" data-cites="Hochreiter:1997:LSM:1246443.1246450">(<a href="#ref-Hochreiter:1997:LSM:1246443.1246450">1997</a>, 10–11)</span>. Meaning that LSTM’s are a good choice for learning time-dependent sequential data. Notable applications range from language translation, video captioning and speech recognition.</p>
<figure title="LSTM unit.">
<a id="fig:lstm"></a>
<img src="./figures/figure_7.svg" alt="Inside an LSTM unit where the gated units, input, output and forget are present." /><figcaption><b class="captiontitle">Figure 1.7: </b>Inside an LSTM unit where the gated units, input, output and forget are present.<span data-label="fig:lstm"></span></figcaption>
</figure>
<p>A breakthrough in 2006 led to the introduction of the <strong>Deep Belief Network</strong> (DBN) introduced by Geoffrey Hinton. DBN’s are generative networks that pioneered a fast learning technique called “Layerwise pre-training&quot; that trains the network unsupervised from a bottom-up approach. “[Intuitively], pre-training is a clever way of initialization&quot; <span class="citation" data-cites="wang:2017">(Wang, Raj, and Xing <a href="#ref-wang:2017">2017</a>, 30)</span>. DBN’s can attain good generalisation results. For example, it achieved an error rate of 1.2% on am MNIST handwritten digit recognition task <span class="citation" data-cites="HinSal06">(Hinton and Salakhutdinov <a href="#ref-HinSal06">2006</a>)</span> in which pre-training was an advantage: “Pretraining helps generalization because it ensures that most of the information in the weights comes from [modelling] the images&quot; <span class="citation" data-cites="HinSal06">(Hinton and Salakhutdinov <a href="#ref-HinSal06">2006</a>, 507)</span>.</p>
<p>Since 2006, interest in deep architectures from the research community rose as computers got faster over time, with neural networks taking advantage of parallel processing, faster GPU’s, and huge amounts of data to break classification records. A deep CNN called “AlexNet&quot; built by <span class="citation" data-cites="Krizhevsky2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-Krizhevsky2012">2012</a>)</span> won the 2012 “ImageNet&quot; challenge with an error rate of 15.3%, surpassing the second best entry error rate, 26.2% and was trained on 2 GPU’s for six days <span class="citation" data-cites="Krizhevsky2012">(<a href="#ref-Krizhevsky2012">2012</a>, 1)</span>. Interestingly, a novel optimisation technique AlexNet uses is called “Dropout“. It speeds up the training process and prevents overfitting by removing neurons from the network. ”Dropout roughly doubles the number of iterations required to converge.&quot; <span class="citation" data-cites="Krizhevsky2012">(Krizhevsky, Sutskever, and Hinton <a href="#ref-Krizhevsky2012">2012</a>, 6)</span>. AlexNet’s success encouraged more deep CNN architectures to be created, such as VGGNet (OxfordNet), GoogLeNet, ResNet, etc.</p>
<p>By glancing at the history of deep learning, it is apparent that there is promise in its potential to solve numerous problems. With inspiration from biology and in combination with the computational power of GPUs, neural networks today are being actively researched, One to consider are <strong>Neural Turing Machines</strong> (NTM) capable of learning basic algorithms such as copying and sorting. What’s more, <span class="citation" data-cites="DBLP:journals/corr/GravesWD14">Graves, Wayne, and Danihelka (<a href="#ref-DBLP:journals/corr/GravesWD14">2014</a>)</span> considers how the NTM resembles a human working memory system by comparing human based rules to simple programs, proposing that the NTM can learn to use it’s own memory. <span class="citation" data-cites="DBLP:journals/corr/GravesWD14">(<a href="#ref-DBLP:journals/corr/GravesWD14">2014</a>, 2)</span>. Despite this technology and more advancements like it being a few years or decades away, the popularity of deep learning remains strong in academia and industry.</p>
<h2 id="sec:history-of-emotion-recognition">History of Emotion Recognition</h2>
<p>It is no surprise that emotion recognition originated from the study of “emotions&quot;. <span class="citation" data-cites="ERIHCI:911197">Cowie et al. (<a href="#ref-ERIHCI:911197">2001</a>)</span> believes that it is examined in three major disciplines: psychology, biology and philosophy <span class="citation" data-cites="ERIHCI:911197">(<a href="#ref-ERIHCI:911197">2001</a>, 35)</span> and in the past had different definitions. Descartes focuses on passions and the soul, he refers to what we call emotions as <em>“the passions&quot;</em> and provides a definition: “...we may define them generally as those perceptions, sensations or emotions of the soul which we refer particularly to it, and which are caused, maintained and strengthened by some movement of the spirits.&quot; <span class="citation" data-cites="descartes_1985">(Descartes <a href="#ref-descartes_1985">1985</a>)</span>. Descartes proceeded to define the six primary passions: <em>“wonder, love, hatred, desire, joy and sadness&quot;</em> omitting the other remaining passions which he contests are related to the primary <span class="citation" data-cites="descartes_1985">(<a href="#ref-descartes_1985">1985</a>, 1:353)</span>. Darwin instead focuses on facial expressions in emotions and argues from a biological perspective. He introduced the idea of “serviceable habits&quot; suggesting that emotions are adaptive. Evidence of this is shown by <span class="citation" data-cites="hess2009darwin">Hess and Thibault (<a href="#ref-hess2009darwin">2009</a>)</span> as they argue that these serviceable habits lost their functionality as humans got more civilised, showing a sign of evolution. <span class="citation" data-cites="hess2009darwin">(<a href="#ref-hess2009darwin">2009</a>, 353)</span>.</p>
<p>The proper classification of distinct emotions was developed by psychologists Ekman and Friesen, where they hypothesised that emotions are universal across all cultures in humans via a stimulus test. The significance of this work indicated that any human could recognise and categorise one of or all of the six basic emotions. “<em>(happiness, sadness, anger, fear, surprise and disgust)</em>&quot; <span class="citation" data-cites="ekman1971constants">(Ekman and Friesen <a href="#ref-ekman1971constants">1971</a>, 124)</span>. This insight also confirms Darwin’s hypothesis of emotion being universal and extends this to animals, as he summarises: “We can thus also understand the fact that the young and the old of widely different races, both with man and animals, express the same state of mind by the same movements.&quot; <span class="citation" data-cites="darwin1872expression">(Darwin <a href="#ref-darwin1872expression">1872</a>, 352)</span>.</p>
<p>The definition of Ekman’s six basic emotions has been the standard benchmark for emotion recognition for computing devices and robots, developing a new field called “Affective Computing&quot; defined as “...computing that relates to, arises from, or deliberately influences emotions.&quot; <span class="citation" data-cites="picard1997affective">(Picard <a href="#ref-picard1997affective">2000</a>)</span>. In the context of facial expression, Picard subscribes to Ekman’s model researching facial expressions and computers attempting to recognise them, “Presently, most attempts to automate recognition of facial expression are based on Ekman’s system.&quot; <span class="citation" data-cites="picard1997affective">(Picard <a href="#ref-picard1997affective">2000</a>)</span></p>
<p>Applications in emotional recognition include a diverse array of areas; such as video games to understand the emotional state of a player playing a game. To online entertainment and marketing, to classify an emotion from a user when watching videos or advertisements to name a few. Emotion Recognition is researched extensively in <strong>Human-Computer Interaction</strong> (HCI) where it can be used to in health care to assess emotional status in patients <span class="citation" data-cites="Lisetti2003245">(Lisetti et al. <a href="#ref-Lisetti2003245">2003</a>)</span> and as an aid in autism, to help children understand emotions around them. Notably, the rise of social media is also playing a role in emotion recognition, one of the most popular applications for it. In their analysis, <span class="citation" data-cites="Roberts:2012ww">K. Roberts et al. (<a href="#ref-Roberts:2012ww">2012</a>)</span> found that from a sample dataset of tweets, most shared either, disgust (16.4%) and joy (12.4%) or no emotion at all (57%) <span class="citation" data-cites="Lisetti2003245">(<a href="#ref-Lisetti2003245">2003</a>, 3808)</span>. By using Ekman’s system as a specification for universal emotion and the introduction of affective computing, there is promise in the future that emotion recognition will show even more promising results in the future.</p>
<h2 id="sec:history-of-animated-cartoons">History of Animated Cartoons</h2>
<p>Cartoons are simplified illustrations drawn to entertain children (comic books and children’s books) to adults (political cartoons and caricatures). Cartoons are considered an extension of an illustrated book and have been recognised as an art form and even a career with the title “Cartoonist&quot;. Despite the fact that cartoons started out in print, attempts to transform them rapidly took place in the early 20th century as “Animated Cartoons&quot;.</p>
<p>Much of the evolution of cartoons becoming animated is credited to many people, combined with their techniques and illusion to mimic the effect of a moving object. Émile Cohl who created the very first hand drawn animation in 1908 called <em>‘Fantasmagorie’</em>. The technique used to create the first full animated cartoon was borrowed from George Méliès a French illusionist and filmmaker who invented the technique of “stop motion&quot;. James Stuart Blackton an illustrator who combined Méliès’s stop motion and Winsor McCay the creator of <em>‘Gertie the Dinosaur’</em> were among one of the first animators in the field.</p>
<p>Regarding technical achievements, <span class="citation" data-cites="yu2013modern">J. Yu and Tao (<a href="#ref-yu2013modern">2013</a>)</span> mentions that Earl Hurd and John Bray created the way of efficiently coordinating the pre-production of an animation listing; composed transparent cels and a peg system for making working with backgrounds easier as examples, that are still in use today in animation <span class="citation" data-cites="yu2013modern">(<a href="#ref-yu2013modern">2013</a>, 4:107)</span>.</p>
<p>From the 1920’s onwards Walt Disney was the most influential pioneers in animation history, alongside his most notable creation and famous mascot “Mickey Mouse&quot;. Disney considered the movies made by him to be experiments, one of them “the usage of sound and colour in animation&quot; <span class="citation" data-cites="yu2013modern">(J. Yu and Tao <a href="#ref-yu2013modern">2013</a>)</span> was in <em>‘Steamboat Willie’</em> in 1928.</p>
<p>Animation historians marked the 1930s as the ‘Golden age of animation’ where animated cartoon came in unison with Hollywood, popular culture and the mainstream through television. Major contributors in this period included “Fleischer, Iwerks, Van Beuren, Universal Pictures, Paramount, [Disney], MGM and Warner Brothers&quot; <span class="citation" data-cites="yu2013modern">(J. Yu and Tao <a href="#ref-yu2013modern">2013</a>)</span>. Onwards into the 1980s, the invention of personal computers moved animated cartoons from paper to pixels. As computers got better with computer graphics, Pixar introduced the first fully computer animated film <em>Toy Story</em>.</p>
<p>Since then, 3D animation became more popular and faster to create for feature length films from the big studios, “and desktop computer animation is now possible at a reasonable cost&quot; <span class="citation" data-cites="yu2013modern">(J. Yu and Tao <a href="#ref-yu2013modern">2013</a>)</span>, with 2D animation getting popular on the internet in the 2000s.</p>
<h2 id="sec:aim">Aim</h2>
<p>It seems that the current progress in both deep learning and emotion recognition suggests that computers are tremendously getting more accurate at correctly classifying emotions in the mediums of videos, speech or images. The state of the art of emotion recognition is tested in the EmotiW (Emotion Recognition in the Wild) Challenge.</p>
<p>The purpose of this project is to measure how accurate a computer can correctly identify an emotion from a given set of images from a cartoon video. <em>This project is an attempt to replicate their success and to find out if these deep learning techniques can be applied to learn specific information in cartoons, the area of interest is emotions</em>. This decision is based on the fact that cartoons are known to express a lot of emotion, especially in the characters, and the choice being ‘animated cartoons’ is one where we can extract emotions from these characters in one or more videos.</p>
<p>The ability for a computer to identify emotions in a cartoon would open more information to be extracted and analysed. Plus, it would explore what is possible with artificial intelligence, emotion recognition and especially animation. <span class="citation" data-cites="wang:2017">(Wang, Raj, and Xing <a href="#ref-wang:2017">2017</a>)</span> admits that “Unfortunately, the computer animation community has not utilised machine learning as widely as computer vision&quot; <span class="citation" data-cites="wang:2017">(Wang, Raj, and Xing <a href="#ref-wang:2017">2017</a>, 3)</span>. Although there has been some research in animation and machine learning, the current state of the art differs greatly and will be discussed in the background section.</p>
<h2 id="sec:objectives">Objectives</h2>
<p>The fulfilment of the above aim of this project: <em>‘to measure how accurate a computer can correctly identify an emotion from a given set of videos’</em> requires the following set of objectives below to be met:</p>
<ol>
<li><p><strong>Discuss relevancy of Deep Neural Networks with relation to Emotion Recognition.</strong></p>
<ul>
<li><p>This just a detailed account of the strengths and weaknesses of the appropriate Deep Neural Networks for this report.</p></li>
</ul></li>
<li><p><strong>Acquire, Clean, Label and Prepare the dataset.</strong></p>
<ul>
<li><p>The dataset will be most likely be generated from manually. e.g. YouTube, otherwise using a pre-trained dataset would suffice due to for time constraints if found.</p></li>
<li><p>If generating the dataset from a YouTube video, a cartoon must be selected for this purpose, with a justification.</p></li>
<li><p>A short account of the facial expressions selected for recognition will be presented.</p></li>
<li><p>A brief discussion of the image processing or computer vision techniques or algorithms will be presented.</p></li>
</ul></li>
<li><p><strong>Design or Select an appropriate Deep Learning model, and create an implementation for training cartoon emotional analysis.</strong></p>
<ul>
<li><p>Depending on time constraints, a minimum of <strong>3 emotions</strong> for emotion recognition for eg. (happy, anger, and suprise). will be considered.</p></li>
<li><p>A short account of the selected software framework and how the model was trained will be discussed.</p></li>
</ul></li>
<li><p><strong>Evaluate the accuracy of the model and finalise the project.</strong></p>
<ul>
<li><p>An analysis of the computer to identify emotions will be measured thoroughly in the evaluation.</p></li>
<li><p>The report ends with a reflection and is finalised.</p></li>
</ul></li>
</ol>
<h2 id="sec:structure-of-the-rest-of-the-report">Structure of the rest of the report</h2>
<p><strong></strong> discusses the background of the report going into a literature review of related work to this project. Some details of the deep learning architectures of the CNN, RNN and even some combinations of the two will be examined. A background and literature review of emotion recognition will be also discussed in this section as well. <strong></strong> provides a short account of the recommended software development methodology chosen for this project plus the research methods. <strong></strong> goes through the <strong>Software Development Life Cycle</strong> (SDLC) phases of the project including the tools used to produce the artefact. <strong></strong> is the penultimate phase that takes into account the optimisation algorithms used to discover the best algorithm for the dataset. The results are included in this section as well. <strong></strong> is a critical reflection of the overall project with a discussion using a risk matrix from the project proposal.</p>
<h1 id="sec:chap:background">Background</h1>
<p>This project intends to link together three fields: animated cartoons, deep learning and emotion recognition. While the latter two fields have been researched in depth, little research of these two latter fields (emotion recognition and deep learning) in the context of animated cartoons have been explored. This chapter sets out to discover the case for why that is and to justify further the purpose of this project.</p>
<h2 id="sec:related-work">Related Work</h2>
<h3 id="sec:emotion-recognition">Emotion Recognition</h3>
<p>For work in the area of emotion recognition, similar research into emotion detection &amp; sentiment analysis in images was conducted by <span class="citation" data-cites="Gajarla:us">(Gajarla and Gupta n.d)</span>. Their dataset was collected from the internet, specifically from online photo sites such as <em>Flickr</em>, <em>Tumblr</em> and <em>Twitter</em>. For the categories to detect emotions they have chosen 5 emotions: “Love, Happiness, Violence, Fear and Sadness.&quot; <span class="citation" data-cites="Gajarla:us">(Gajarla and Gupta n.d, 2)</span>. Some pre-trained CNN models were tested. VGG-ImageNet, VGG-Places205 and a ResNet-50 model were fine tuned to detect emotions in the dataset. They found that the ResNet-50 model produced a result of 73% accuracy showing promise of only a fine tuned model. Interestingly, for the emotions ‘Sadness’ and ‘Happiness’ the model is able to learn faces from the dataset. However, “We also observe that 80% of the images that are tagged as happiness are face images. Hence this category is biased towards faces.&quot; <span class="citation" data-cites="Gajarla:us">(Gajarla and Gupta n.d, 3)</span>. Alternatively, the dataset for this project will consist of only faces showing facial expressions to keep the dataset fair.</p>
<p>With any dataset, it may be worthwhile to see how other well established datasets perform in relation to other similar datasets in emotion recognition, in addition to learn how the dataset was constructed. <span class="citation" data-cites="Zafeiriou:2016kn">(Zafeiriou et al. <a href="#ref-Zafeiriou:2016kn">2016</a>)</span> focused on surveying databases that have faces collected “in the wild&quot;. That is, datasets that contain faces not produced in a strictly controlled setting, rather just publicly available faces hand annotated to a specific emotion. The datasets of interest to this project are datasets that contain facial expressions, <span class="citation" data-cites="Zafeiriou:2016kn">Zafeiriou et al. (<a href="#ref-Zafeiriou:2016kn">2016</a>)</span> attributes the most prominent datasets <strong>Facial Expression Recognition 2013</strong> (FER-2013) which was collected using Google Images and was constructed as greyscale 48 <span class="math inline">\( \times \)</span> 48 faces containing the universal expressions, in addition with the neutral emotion with a total of 35,887 images. Both the <strong>Acted Facial Expression In The Wild</strong> (AFEW) and Static Facial Expression In The Wild (SFEW) datasets were used in the Emotion Recognition “in-the-wild&quot; challenges <span class="citation" data-cites="Zafeiriou:2016kn">(<a href="#ref-Zafeiriou:2016kn">2016</a>, 1490)</span>, in relation to this, <span class="citation" data-cites="Kahou:2015cr">Kahou et al. (<a href="#ref-Kahou:2015cr">2015</a>)</span> used the FER-2013 dataset alongside additional datasets, such as the <strong>Toronto Face Dataset</strong> (TFD) and AFEW <span class="citation" data-cites="Kahou:2015cr">(<a href="#ref-Kahou:2015cr">2015</a>, 468)</span>.</p>
<p>However, <span class="citation" data-cites="Zafeiriou:2016kn">(Zafeiriou et al. <a href="#ref-Zafeiriou:2016kn">2016</a>)</span> mentions that the AFEW and SFEW datasets only contain posed facial expressions from motion pictures that are annotated to only the universal expressions proposed by <span class="citation" data-cites="ekman1971constants">(Ekman and Friesen <a href="#ref-ekman1971constants">1971</a>)</span>, stating that “...[its] a taxonomy rarely that is considered too limited for modelling real-world emotional state&quot;. This observation was a challenge faced by both <span class="citation" data-cites="Gajarla:us">(Gajarla and Gupta n.d)</span> “The problem of labelling images with the emotion they depict is very subjective and can differ from person to person.&quot; <span class="citation" data-cites="Gajarla:us">(Gajarla and Gupta n.d, 4)</span> and also <span class="citation" data-cites="Kahou:2015cr">(Kahou et al. <a href="#ref-Kahou:2015cr">2015</a>)</span> “We found that a fairly large number of training videos could be argued to show a mixture of two or more basic emotions&quot; <span class="citation" data-cites="Kahou:2015cr">(Kahou et al. <a href="#ref-Kahou:2015cr">2015</a>, 47)</span>. Extra care is needed for animated cartoons since cartoons can display various emotions that can be ambiguous or even comprise of two different emotions. As a result the ‘neutral’ emotion will not be classified in this project.</p>
<p>Applications in emotion recognition as <span class="citation" data-cites="ERIHCI:911197">Cowie et al. (<a href="#ref-ERIHCI:911197">2001</a>)</span> points out, include avoidance, alerting, production, tutoring and entertainment. This project would most likely fall in-between the applications of tutoring and entertainment. The former, because it could allow a computer to recognise an emotion in an animated cartoon automatically. It could generate subtitles (text or audio) explaining and teaching the emotions of the characters throughout the video to children. The latter for the fact that cartoons is a form of entertainment; to adults and especially children. For example, a recommendation system could be envisioned where an animated cartoon has a emotion rating outlining which characters possess various emotions in an episode.</p>
<p>There will be issues with these applications, mainly with the systems that need to detect emotions, whether through speech or faces. <span class="citation" data-cites="ERIHCI:911197">Cowie et al. (<a href="#ref-ERIHCI:911197">2001</a>)</span> argues that deception is a hard challenge in detecting for computers <span class="citation" data-cites="ERIHCI:911197">(<a href="#ref-ERIHCI:911197">2001</a>)</span>. Since humans are capable of this, cartoons can also have characters that show forms of deception, which can confuse a system trying to recognise an emotion. Hence, “...that means accepting that the system will be deceived as humans are&quot; <span class="citation" data-cites="ERIHCI:911197">(Cowie et al. <a href="#ref-ERIHCI:911197">2001</a>)</span>.</p>
<h3 id="sec:animated-cartoons">Animated Cartoons</h3>
<p>There exists some research in the area of machine learning and animated cartoons, however this research does not include any deep learning methods. Nevertheless, <span class="citation" data-cites="wang:2017">Wang, Raj, and Xing (<a href="#ref-wang:2017">2017</a>)</span> mentions that manifold learning is a popular machine learning technique for animated cartoons and maintains a close relationship with animation research <span class="citation" data-cites="wang:2017">(<a href="#ref-wang:2017">2017</a>, 3)</span>. Manifold learning is a unsupervised learning method that transforms high dimensional data into a low dimension. The idea of this transformation is attractive in machine learning because “It aims to reveal the intrinsic structure of the distribution measurements in the original high dimensional space&quot; <span class="citation" data-cites="wang:2017">(Wang, Raj, and Xing <a href="#ref-wang:2017">2017</a>)</span>. <span class="citation" data-cites="deJuan:2004ex">de Juan and Bodenheimer (<a href="#ref-deJuan:2004ex">2004</a>)</span> applied this by reusing existing cartoon data and re-sequencing the cartoon frames into a new animation, the low dimension manifold serves as a similarity graph of each frame of a cartoon <span class="citation" data-cites="deJuan:2004ex">(<a href="#ref-deJuan:2004ex">2004</a>)</span>. Both multi-layer neural networks and manifold learning can represent data in a non-linear fashion and according to <span class="citation" data-cites="Rajanna:2016ux">Rajanna et al. (<a href="#ref-Rajanna:2016ux">2016</a>)</span> can be combined together to boost classification accuracy <span class="citation" data-cites="Rajanna:2016ux">(<a href="#ref-Rajanna:2016ux">2016</a>)</span>. While manifold learning is out of the scope of this report, deep architectures such as Deep Belief Networks (stacked <em>Restricted Boltzmann Machines</em> (RBMs)) and Autoencoders could be explored further for animation.</p>
<h2 id="sec:convolutional-neural-networks">Convolutional Neural Networks</h2>
<p>Both animals and humans are extremely good at visual tasks, in fact <span class="citation" data-cites="Russakovsky:2014vi">Russakovsky et al. (<a href="#ref-Russakovsky:2014vi">2014</a>)</span> concluded from an evaluation of the ILSVRC’s 5 year history that humans achieved a classification error of 5.1% from an assortment of 1500 images <span class="citation" data-cites="Russakovsky:2014vi">(<a href="#ref-Russakovsky:2014vi">2014</a>, 31)</span>. In the past, computers were unable to perform these visual tasks until the introduction of the CNN.</p>
<p>The CNN is a special type of neural network, it was first researched in the area of neuroscience with the inspiration of the animal visual cortex by <span class="citation" data-cites="hubel1962receptive">(Hubel and Wiesel <a href="#ref-hubel1962receptive">1962</a>)</span>. <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 247)</span> describes one of the many factors that made the CNN successful in computer vision tasks. Given a single image of a cat for example, skewing or distorting this image does not change the fact that the image contains a cat <span class="citation" data-cites="Goodfellow-et-al-2016">(<a href="#ref-Goodfellow-et-al-2016">2016</a>, 247)</span>. Humans understand this intuitive principle well, and so do CNN’s, this property that CNN’s have is called <em>shift invariance</em>, the idea that an algorithm can distinguish features in an image; regardless if the image is shifted in large or small orders of magnitude. “CNNs take this property into account by sharing parameters across multiple image locations.&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 247)</span>, and in return this returns a great benefit for the network in that it “...has enabled CNN’s to dramatically lower the number of unique model parameters&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 247)</span>. In comparison to a fully connected network, (typical FFNN) overfitting is less likely in a CNN, because only local neurons are connected to each other and in turn the number of parameters are reduced. Figure <a href="#fig:shared_weights">2.1</a> shows an illustration of parameter sharing, only local neurons that are close together share weights such as <span class="math inline">\( f_1 \)</span> &amp; <span class="math inline">\( f_2 \)</span>. <span class="citation" data-cites="Goodfellow-et-al-2016">Goodfellow, Bengio, and Courville (<a href="#ref-Goodfellow-et-al-2016">2016</a>)</span> recognises this added benefit, such that the network only has to learn one set of parameters rather than a separate set of parameters for each weight and is in turn computationally efficient <span class="citation" data-cites="Goodfellow-et-al-2016">(<a href="#ref-Goodfellow-et-al-2016">2016</a>, 328)</span>.</p>
<figure title="An illustrated example of parameter sharing.">
<a id="fig:shared_weights"></a>
<img src="./figures/figure_8.svg" /><figcaption><b class="captiontitle">Figure 2.1: </b>An illustrated example of parameter sharing (also called weight sharing). Each weight is shared to each feature map <span class="math inline">\( f_n \)</span>, this reduces the number of parameters in the network.<span data-label="fig:shared_weights"></span></figcaption>
</figure>
<p>As the name suggests, the word <em>‘Convolution’</em> is indeed derived from a mathematical operation, requiring two functions <span class="math inline">\( f \)</span> &amp; <span class="math inline">\( g \)</span> and producing another function <span class="math inline">\( h \)</span> which is an integration of the amount of overlap of <span class="math inline">\( f \)</span> as it is shifted over <span class="math inline">\( g \)</span> <span class="citation" data-cites="wang:2017">(Wang, Raj, and Xing <a href="#ref-wang:2017">2017</a>, 36)</span>. Equation <a href="#eq:2.1">2.1</a> describes this operation for a single dimension. Traditionally, convolving the functions <span class="math inline">\( f \)</span> &amp; <span class="math inline">\( g \)</span> is denoted as <span class="math inline">\( (f * g) \)</span> although other notations <span class="math inline">\( (f \otimes g) \)</span> or <span class="math inline">\( (f \circledast g) \)</span> are sometimes used as well, mostly in signal processing. The formal notation will be used instead of the latter notations.</p>
<div>
<a id="eq:2.1"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:6}
    h(t) = (f * g)(t) = \int_{-\infty}^{\infty}f(\tau)g(t-\tau)\ \mathrm{d}\tau\]</span></td>
<td style="text-align: right;"><span class="math display">\[(2.1)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>This operation has the property of commutativity, that is: <span class="math inline">\( (f * g) = (g * f) \)</span>. Along with the definition of convolution in Equation <a href="#eq:2.1">2.1</a>, To prove this, a change in the integration variable <span class="math inline">\( \tau \)</span> to become <span class="math inline">\( \tau \rightarrow t - \tau \)</span>, thus this is equivalent to:</p>
<div>
<a id="eq:2.2"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:7}
    h(t) = (f * g)(t) = \int_{-\infty}^{\infty}g(t-\tau)f(\tau)\ \mathrm{d}\tau\]</span></td>
<td style="text-align: right;"><span class="math display">\[(2.2)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>This proves that <span class="math inline">\( (f * g) = (g * f) \)</span> only for the functions <span class="math inline">\(f, g \in \mathbb{R}^n \)</span> <span class="citation" data-cites="Strichartz:2003tk">(Strichartz <a href="#ref-Strichartz:2003tk">2003</a>, 32)</span>, this property is just a reverse operation of a signal but the reason for this will become clear later on.</p>
<p>In the context of CNN’s, one of the ways the convolution operation is best understood is in the domain of digital images. Thus, we will consider focusing on convolving over discrete functions rather than continuous functions such as analog signals in Equation <a href="#eq:2.1">2.1</a>. The type of convolution needed for this kind of data is called a discrete convolution as shown in Equation <a href="#eq:2.3">2.3</a>, the only difference to this equation is that the integral is now switched for the summation operator for digitised data, and that we use integers (<span class="math inline">\( \mathbb{Z} \)</span>) instead of real numbers (<span class="math inline">\( \mathbb{R} \)</span>).</p>
<div>
<a id="eq:2.3"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:8}
    h(t) = (f * g)(t) = \sum_{k={-\infty}}^{\infty}f(k)g(t-k)\]</span></td>
<td style="text-align: right;"><span class="math display">\[(2.3)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>However, this is still a one dimensional convolution; to work with digital images it is ideal to perform a 2D convolution. Such a convolution comprises of three parts, both of which apply to both images and CNNs: “...the first argument (in this example [<a href="#eq:2.3">2.3</a>], the function [f]) to the convolution is often referred to as the input and the second argument (in this example, the function [g]) as the kernel. The output is sometimes referred to as the feature map.&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 322–23)</span>. In addition to the equation in Equation <a href="#eq:2.3">2.3</a>, we would need to introduce a double summation to accommodate the matrix rows &amp; columns to convolve the kernel on. The result is Equation <a href="#eq:2.4">2.4</a> in which the commutative property also holds true if <span class="math inline">\( I(m,n) \rightarrow I(i-m, j-n)  \)</span> and <span class="math inline">\( K(i-m, j-n) \rightarrow K(m,n) \)</span>.</p>
<div>
<a id="eq:2.4"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:9}
    H(i,j) = (I * K)(i,j) = \sum_{m} \sum_{n} I(m,n)K(i-m, j-n).\]</span></td>
<td style="text-align: right;"><span class="math display">\[(2.4)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>“...the only reason to flip the kernel is to obtain the commutative property&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 323)</span>. One should take caution that the similar operation called cross correlation is the same as convolution without flipping the kernel <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 324)</span>. The explanation of cross correlation is out of the scope of this article but Equation <a href="#eq:2.5">2.5</a> demonstrates how similar it is to convolution. Another difference is that cross-correlation does not have a commutative property whereas convolution does.</p>
<div>
<a id="eq:2.5"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:10}
    H(i,j) = (I * K)(i,j) = \sum_{m} \sum_{n} I(m,n)K(i+m, j+n).\]</span></td>
<td style="text-align: right;"><span class="math display">\[(2.5)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>The important difference between the two is that convolution describes the <em>modification</em> of a signal whereas cross-correlation describes the <em>similarity</em> of a signal. Convolving the kernel <span class="math inline">\( K \)</span> over image <span class="math inline">\( I \)</span> happens by sliding the kernel <span class="math inline">\( K \)</span> over the input image for each row and column in the input image. The output of this operation from the kernel convolution is called a feature map. The three examples of this process are shown in Figure <a href="#fig:2d_convolution">2.2</a> where the example kernel is a laplacian kernel mostly used in image processing to highlight edges in images.</p>
<figure title="An example of a 2D convolution.">
<a id="fig:2d_convolution"></a>
<img src="./figures/figure_9.svg" /><figcaption><b class="captiontitle">Figure 2.2: </b>An example of a 2D convolution using an edge detection kernel.<span data-label="fig:2d_convolution"></span></figcaption>
</figure>
<p>Different kernels for 2D images can be used depending on what feature the CNN has to recognise, but what is certain is that the next stage after creating our feature map is to remove irrelevant features in the image. The CNN has a dedicated layer for this, called the pooling/subsampling layer. The idea behind this layer is to reduce the dimensions of the image and only keep the important features for further processing.</p>
<figure title="Max pooling.">
<a id="fig:maxpool"></a>
<img src="./figures/figure_10.svg" /><figcaption><b class="captiontitle">Figure 2.3: </b>An example of dimensionality reduction, the input image is reduced with a 9x9 Max Pooling, note that the image is resized and downsampled after this stage.<span data-label="fig:maxpool"></span></figcaption>
</figure>
<p>Max pooling is a common example used by CNN’s in dimensionality reduction. It simply returns the maximum pixel of an image from a region of <span class="math inline">\( M \times N \)</span> pixels. Max pooling is not the only technique in reducing features, “Other popular pooling functions include the average of a rectangular neighbourhood, the <span class="math inline">\(L^2\)</span> norm of rectangular neighbourhood, or a weighted average based on the distance from the central pixel.&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 330)</span>. Figure <a href="#fig:maxpool">2.3</a> shows an example of max pooling taking place on two images. This particular reduction layer is responsible for the CNN being invariant to small translations, <span class="citation" data-cites="Goodfellow-et-al-2016">Goodfellow, Bengio, and Courville (<a href="#ref-Goodfellow-et-al-2016">2016</a>)</span> highlights this point, suggesting that in all cases pooling helps to transform the representation of an image to become invariant to small translations of the input. <span class="citation" data-cites="Goodfellow-et-al-2016">(<a href="#ref-Goodfellow-et-al-2016">2016</a>, 330)</span>. This powerful but simple technique means that no matter the image input, pooling deconstructs the image into a smaller representation that allows the network to only focus on the most important features even with a small shift of the input image. “Invariance to local translation can be a useful property if we care more about whether some feature is present than exactly where it is.&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 331)</span>. <span class="citation" data-cites="Goodfellow-et-al-2016">Goodfellow, Bengio, and Courville (<a href="#ref-Goodfellow-et-al-2016">2016</a>)</span> goes further to provide examples of the features that CNN’s look for regardless of location, such as whether an image contains a face or eyes, or corners of some edges we only need to know if a face or an edge <strong>exists</strong> and not to worry about the location of such features. <span class="citation" data-cites="Goodfellow-et-al-2016">(<a href="#ref-Goodfellow-et-al-2016">2016</a>, 331)</span>.</p>
<p>If we take the example in Figure <a href="#fig:maxpool">2.3</a>, the letter ‘<em>A</em>’ after it is convolved and pooled, shows a large highlighted region of white pixels. The CNN would most likely look for those edges to determine if it really is the letter ‘<em>A</em>’. These edges are the features that the CNN would look for that are similar to Figure <a href="#fig:shared_weights">2.1</a> most likely feature <em><span class="math inline">\(f_1\)</span></em>. After many layers of pooling and convolutions, the representation of the image would be reduced to the point that the image would not be recognisable prima facie. For this reason fully connected layers connect the reduced image pixels from the previous layer to a layer where neurons are interconnected to each other rather than sparse connections like CNN layers. We can get a better understanding of what sort of features the network will learn from the individual pixels in the fully connected layer.</p>
<figure title="Fully Connected Network & ReLU.">
<a id="fig:fully_connected_output_layers_relu"></a>
<span> <img class="side_img" width="280" src="./figures/figure_11.svg"/></span>
<span> <img class="side_img" width="280" src="./figures/figure_12.svg"/></span>
<p><span class="math display">
    <a id="eq:2.6"></a>
                \[\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}} \ \ \ \ \ \forall j \in 1...N  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ x = max(0, x) \] 
</span><figcaption><b class="captiontitle">Figure 2.4: </b>(Left) an illustrated example of two fully connected networks with an output layer
and the class prediction labels. (Right) illustration of the ReLU activation function.<span data-label="fig:fully_connected_output_layers_relu"></span></figcaption>
</figure>
</p>
<p>The last layer of the network is called the output layer (also called the classification layer) lists all the predictions of what the input could be. Since the input could be either ‘<em>A</em>’, ‘<em>B</em>’ or ‘<em>C</em>’ we would need at least 26 output neurons to classify a single character from the alphabet (assuming only uppercase letters). Figure <a href="#fig:fully_connected_output_layers_relu">2.4</a> shows an example of two fully connected layers and an activation function called a rectified linear unit or (ReLU). ReLU differs from the sigmoid function (see Equation <a href="#eq:1.5">1.5</a>) in that it is not upper bounded between 0 or 1. But they are both similar in that their activation response is always non-negative, because both of these functions are always lower bounded at 0.</p>
<p>There is an advantage for being strictly lower bounded to 0 in the case for the ReLU activation function, <span class="citation" data-cites="Glorot:2011tm">Glorot, Bordes, and Bengio (<a href="#ref-Glorot:2011tm">2011</a>)</span> argues that ReLU introduces a sparsity property, meaning that neurons with negative activations turn into ‘real zeros’ that simply will not activate and pass through the network and can make data more linearly separable <span class="citation" data-cites="Glorot:2011tm">(<a href="#ref-Glorot:2011tm">2011</a>, 317)</span>. ReLU is truly closer to the activations inside a biological neruon as opposed to the sigmoid function, “...rectifier units naturally lead to sparse networks and are closer to biological neurons‘ responses in their main operating regime.&quot; <span class="citation" data-cites="Glorot:2011tm">(Glorot, Bordes, and Bengio <a href="#ref-Glorot:2011tm">2011</a>, 316)</span>. This has lead to the result that : “...training proceeds better when the artificial neurons are either off or operating mostly in a linear regime.&quot; <span class="citation" data-cites="Glorot:2011tm">(Glorot, Bordes, and Bengio <a href="#ref-Glorot:2011tm">2011</a>, 316)</span>. Equation <a href="#eq:2.6">2.6</a> on the right shows the ReLU formula taking only the maximum value from 0 to <span class="math inline">\( x \)</span>.</p>
<p>But with all activation functions, <span class="citation" data-cites="Glorot:2011tm">(Glorot, Bordes, and Bengio <a href="#ref-Glorot:2011tm">2011</a>)</span> issues a caution of ReLU: “...forcing too much sparsity may hurt predictive performance for an equal number of neurons, because it reduces the effective capacity of the model&quot; <span class="citation" data-cites="Glorot:2011tm">(Glorot, Bordes, and Bengio <a href="#ref-Glorot:2011tm">2011</a>, 316)</span>. But despite this, ReLU is one of the most popular activation functions in use in deep neural networks. “Deep convolutional neural net- works with ReLUs train several times faster than their equivalents with tanh units.&quot; <span class="citation" data-cites="Krizhevsky2012">(Krizhevsky, Sutskever, and Hinton <a href="#ref-Krizhevsky2012">2012</a>)</span></p>
<p>For classification in the output layer, the softmax function is mostly used. Although softmax is visually similar to the sigmoid, they are dissimilar in function. The softmax function translates an input vector <span class="math inline">\( v \)</span> into an array of probabilities for each vector. A good case for using this type of activation function is for multi-class classification rather than binary classification like sigmoid. For example in the case for Figure <a href="#fig:softmax">2.5</a>, a vector of prediction probabilities for each character in the alphabet is returned when passed through the softmax function, the probabilities determines the most likely class label to be predicted. Equation <a href="#eq:2.6">2.6</a> on the left shows the softmax equation.</p>
<figure title="An illustrated example of the softmax function.">
<a id="fig:softmax"></a>
<img src="./figures/figure_13.svg" /><figcaption><b class="captiontitle">Figure 2.5: </b>An illustrated example of the softmax function. Adding all the probabilities sums 1 for every character in the class label.<span data-label="fig:softmax"></span></figcaption>
</figure>
<p>This process repeats (from convolution to softmax) for a number of times until the maximum steps (or epochs) is reached. The first pass of the training process is known as a forward propagation, for brevity of this report, the backpropagation algorithm is not explained, rather the gradient descent optimisation algorithms that accompany it are in the testing &amp; evaluation section.</p>
<p>It should also be worth noting that CNN’s are not only used in image based problems, <span class="citation" data-cites="Hu:2015uo">Hu et al. (<a href="#ref-Hu:2015uo">2015</a>)</span> applied CNN’s to <strong>Natural Language Processing</strong> (NLP) by proposing a convolutional sentence matching model where the meaning of a sentence is summarised by performing convolution and pooling <span class="citation" data-cites="Hu:2015uo">(<a href="#ref-Hu:2015uo">2015</a>)</span>. CNN’s have become more complex than the ones shown in Figure <a href="#fig:convnet">1.5</a> and Figure <a href="#fig:softmax">2.5</a>, containing multiple subsampling and convolutional layers depending on the size of the dataset and problem.</p>
<h2 id="sec:imagenet">ImageNet</h2>
<p>The annual ImageNet challenge, (formally the ILSVRC challenge) was designed to assess the state of the art of computer vision by achieving a high degree of accuracy in image classification and object recognition. The dataset consists of over 14 million images (as hyperlinks) in more than 21,000 categories. The goal of the challenge is to classify 1000 images in 1000 categories from using a small portion of images from the ImageNet dataset.</p>
<p><em>ImageNet Classification with Deep Convolutional Neural Networks</em> (or known as the ‘AlexNet’ paper) authored by <span class="citation" data-cites="Krizhevsky2012">(Krizhevsky, Sutskever, and Hinton <a href="#ref-Krizhevsky2012">2012</a>)</span> was a turning point in the ImageNet challenge that produced a winning error rate of 15.3%, beating the previous record which was 26.2%. Modern deep convolutional neural network architectures today improve and surpass AlexNet, such as VGGNet (OxfordNet), GoogLeNet, ResNet and the latest one to date, CUImage holds the record of 2.66%. The choice of this paper was down to the fact that this was where the CNN was first used (and continues to be used) in this challenge and serves as the benchmark for image classification.</p>
<p>AlexNet’s CNN architecture and others that are similar to it are worth exploring consisting of “...eight layers with weights; the first five are convolutional and the remaining three are fully-connected.&quot; <span class="citation" data-cites="Krizhevsky2012">(Krizhevsky, Sutskever, and Hinton <a href="#ref-Krizhevsky2012">2012</a>)</span>. <span class="citation" data-cites="Krizhevsky2012">Krizhevsky, Sutskever, and Hinton (<a href="#ref-Krizhevsky2012">2012</a>)</span> outlines the main architecture, the layer convolves the 224 <span class="math inline">\( \times \)</span> 224 <span class="math inline">\( \times \)</span> 3 input image with 96 kernels with size 11 <span class="math inline">\( \times \)</span> 11 <span class="math inline">\( \times \)</span> 3 and the image is pooled. The layer convolves the first layer with 256 kernels with size 5 <span class="math inline">\( \times \)</span> 5 <span class="math inline">\( \times \)</span> 48 that are then subsequently pooled. The , , contain the same width and height (3 <span class="math inline">\( \times \)</span> 3) but with various depths, 256, 192, 192. The fully connected layers have 4096 neurons each with the output layer containing 1000 nerons. <span class="citation" data-cites="Krizhevsky2012">(<a href="#ref-Krizhevsky2012">2012</a>)</span>. Figure <a href="#fig:alexnet">2.6</a> shows a simplified version of AlexNet. The network was split across two GPUs because one GPU was not enough to accommodate training 1.2 million training examples, It did have an advantage though: “The two-GPU net takes slightly less time to train than the one-GPU net.&quot; <span class="citation" data-cites="Krizhevsky2012">(Krizhevsky, Sutskever, and Hinton <a href="#ref-Krizhevsky2012">2012</a>)</span>.</p>
<figure title="AlexNet architecture.">
<a id="fig:alexnet"></a>
<img src="./figures/figure_14.svg" /><figcaption><b class="captiontitle">Figure 2.6: </b>An illustrated and simplified example of the AlexNet architecture.<span data-label="fig:alexnet"></span></figcaption>
</figure>
<div>
<a id="tab:zebra_results"></a>
<table align="center">
<thead>
<tr class="header">
<th style="text-align: left;">Class</th>
<th style="text-align: left;">Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>zebra</strong></td>
<td style="text-align: left;"><strong>0.91752</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">hartebeest</td>
<td style="text-align: left;">0.00153</td>
</tr>
<tr class="odd">
<td style="text-align: left;">impala, Aepyceros melampus</td>
<td style="text-align: left;">0.00104</td>
</tr>
<tr class="even">
<td style="text-align: left;">ostrich, Struthio camelus</td>
<td style="text-align: left;">0.00099</td>
</tr>
<tr class="odd">
<td style="text-align: left;">prairie chicken, prairie grouse, prairie fowl</td>
<td style="text-align: left;">0.00075</td>
</tr>
</tbody>
</table>
<figcaption align="middle"><b class="captiontitle">Table 2.1: </b>ImageNet classification of the zebra by <span class="citation" data-cites="randomlists.com">(Randomlists.com n.d)</span> in Figure <a href="#fig:alexnet">2.6</a>.<span data-label="tab:zebra_results"></span></figaption>
</div>
<p>With the recent great successes of ImageNet classification, and alongside the introduction of the CNN in this challenge thanks to AlexNet, classifying images such as animals is very accurate and works very well. However, for the purpose of this project, the ImageNet database does not contain any cartoons as of writing. But the correct classification of some cartoons is minimal, although slightly promising. This has been tested on a sample of some cartoon images with the Inception model shown in Figure <a href="#fig:inception">2.7</a> returning results with varying degrees of accuracy. One might argue that using transfer learning may help to alleviate the cartoon dataset, transfer learning is applying a pre-trained model onto another problem or domain. Transfer learning is effective for cartoons that contain single objects, shown in Figure <a href="#fig:inception">2.7</a>, however with some cartoon images where the cartoon is embedded in a setting it becomes near to ineffective. Furthermore, any model pre-trained on the ImageNet dataset is likely to incorrectly classify a cartoon object. For example a chair inside a cartoon would be harder to distinguish for a pre-trained ImageNet model than a cartoon chair alone, different architectures other than a CNN or training a whole new cartoon dataset needs to be taken into consideration.</p>

<h2 id="sec:recurrent-neural-networks">Recurrent Neural Networks</h2>
<p>Recurrent Neural Networks (RNNs) have shown to have great advantages on sequential data such as videos or speech, they work by “...[processing] an input sequence one element at a time, maintaining in their hidden units a ‘state vector’ that implicitly contains information about the history of all the past elements of the sequence.&quot; <span class="citation" data-cites="LeCun:2015dt">(LeCun Y., Bengio, and Hinton <a href="#ref-LeCun:2015dt">2015</a>)</span> Compared to a traditional FFNN, RNN’s take advantage of parameter sharing, just like the CNN, but differently. <span class="citation" data-cites="Goodfellow-et-al-2016">Goodfellow, Bengio, and Courville (<a href="#ref-Goodfellow-et-al-2016">2016</a>)</span> points that the weights are shared across several timesteps <span class="citation" data-cites="Goodfellow-et-al-2016">(<a href="#ref-Goodfellow-et-al-2016">2016</a>)</span>. “Each member of the output is a function of the previous members of the output. Each member of the output is produced using the same update rule applied to the previous outputs.&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>)</span>, there is even a possibility they can be used in combination with CNN’s to achieve high accuracy results in classification.</p>

<figure title="ImageNet classification on Cartoons.">
<a id="fig:inception"></a>
<img src="./figures/figure_16.svg" /><figcaption><b class="captiontitle">Figure 2.7: </b>Classification of some cartoon images of chairs and animals. The model correctly classifies the orange couch ( image <span class="citation" data-cites="openclipart_vectors:2013">(Vectors <a href="#ref-openclipart_vectors:2013">2013</a>)</span>) with <code>98%</code> accuracy, and also classifies the image <span class="citation" data-cites="britton:2017">(Britton <a href="#ref-britton:2017">2017</a>)</span> as a <code>rocking chair</code> at 35% accuracy. However, apart from the image above, the percentages for all the other images are less than 50%. On two occasions, the ( <span class="citation" data-cites="shiplett:2013">(Shiplett <a href="#ref-shiplett:2013">2013</a>)</span> &amp; <span class="citation" data-cites="warner_bros:2013">(Warner Bros <a href="#ref-warner_bros:2013">2013</a>)</span>) images are misclassified as a <code>comic book</code>. The image <span class="citation" data-cites="rayner">(Rayner n.d)</span> is narrowly misclassified as a nail and the <span class="citation" data-cites="belortaja:2009">(Belortaja <a href="#ref-belortaja:2009">2009</a>)</span> image is the lowest scoring image that misclassifies the cat with a <code>jigsaw puzzle</code> at 13% accuracy. © Warner Bros. Entertainment, Inc<span data-label="fig:inception"></span></figcaption>
</figure>

<p><span class="citation" data-cites="Kahou:2015cr">(Kahou et al. <a href="#ref-Kahou:2015cr">2015</a>)</span> proposed a hybrid CNN-RNN architecture to predict facial expressions, plus they outline the inner workings of their architecture: “In the first step, [a] CNN is trained to classify static images containing emotions. In the second step, we train an RNN on the higher layer representation of the CNN inferred from individual frames to predict a single emotion for the entire video.&quot; <span class="citation" data-cites="Kahou:2015cr">(Kahou et al. <a href="#ref-Kahou:2015cr">2015</a>, 2)</span>. For the RNN component of the network, the LSTM was not considered, although they were aware of the vanishing/exploding gradient problem in RNNs. Instead, a specific RNN architecture called an <strong>Identity Recurrent Neural Network</strong> (IRNN) proposed by <span class="citation" data-cites="Le:2015vt">(Le, Jaitly, and Hinton <a href="#ref-Le:2015vt">2015</a>)</span> performs a weight initialisation trick to an identity matrix. Given this type of initialisation, “...an RNN that is composed of ReLUs and initialized with the identity matrix...just stays in the same state indefinitely...when the error derivatives for the hidden units are backpropagated through time they remain constant provided no extra error-derivatives are added.&quot; <span class="citation" data-cites="Le:2015vt">(Le, Jaitly, and Hinton <a href="#ref-Le:2015vt">2015</a>, 2)</span>. This behaviour can have the effect of performing similarly to LSTMs as they further state: “Their performance on test data is comparable with LSTMs, both for toy problems involving very long-range temporal structures and for real tasks like predicting the next word in a very large corpus of text.&quot;<span class="citation" data-cites="Le:2015vt">(Le, Jaitly, and Hinton <a href="#ref-Le:2015vt">2015</a>, 2)</span>. To summarise, using an IRNN architecture could be a simpler alternative to an LSTM, if one needs to tackle the long term dependencies problem that faces traditional RNNs. However, since <span class="citation" data-cites="Kahou:2015cr">(Kahou et al. <a href="#ref-Kahou:2015cr">2015</a>)</span> did not test LSTM performance with an IRNN, IRNN might be a lesser choice as found by <span class="citation" data-cites="Talathi:2015uv">Talathi and Vartak (<a href="#ref-Talathi:2015uv">2015</a>)</span> which concluded that IRNN’s have hidden nodes are very sensitive due to it’s identity weight matrix initialisation thus resulting in varying hyperparameters for successful training <span class="citation" data-cites="Talathi:2015uv">(<a href="#ref-Talathi:2015uv">2015</a>)</span>. On the other hand LSTM’s are arguably more complex than IRNNs and standard RNNs but achieve better results. In terms of emotion classification, the hybrid CNN-RNN architecture achieves a test accuracy of 52%. While that CNN-RNN architecture is novel in of itself, it is very complex alone for this project and even may introduce overfitting despite it’s proven ability to classify emotions. <span class="citation" data-cites="Kahou:2015cr">Kahou et al. (<a href="#ref-Kahou:2015cr">2015</a>)</span> recognised this in their due in part to the deep structure of the CNN, and reduced the network to a 3 layer network that alleviated the overfitting issue <span class="citation" data-cites="Kahou:2015cr">(<a href="#ref-Kahou:2015cr">2015</a>)</span>.</p>
<h1 id="sec:chap:methodology">Methodology</h1>
<p>This chapter covers the planning and the choice of software development methodology for the project. In addition to the research methods conducted to evaluate the model. This research is a planned account of what will be discussed in the evaluation section.</p>
<h2 id="sec:project-management">Project Management</h2>
<h3 id="sec:trello">Trello</h3>
<p>To aid the development of the artefact, the project management application <em>‘Trello’</em> was used to track the software development process, and in addition to this report as well. It is an online project planning tool that respects the common Kanban workflow in software projects. This workflow consists of primarily 3 main boards, <em>Todo</em>, <em>Doing</em> and <em>Done</em>, this is shown in Figure <a href="#fig:trello">3.1</a> showing the standard three boards in <em>Trello</em>.</p>
<figure title="Trello Board.">
<a id="fig:trello"></a>
<img src="./figures/figure_17.svg" /><figcaption><b class="captiontitle">Figure 3.1: </b>An example of a <em>Trello</em> board with the 3 main Kanban boards.<span data-label="fig:trello"></span></figcaption>
</figure>
<p>But depending on the project and the team, more boards than the standard three can be added if required. In the context of this project, the project includes 5 boards, two of them are both in the <em>Todo</em> section: <em>Report (Todo)</em>, <em>Artefact (Todo)</em>, <em>Working on (This Week)</em>, <em>Finished</em> and <em>Future Goals</em>. The <em>Working on (This Week)</em> is key to the management of this project in Kanban. Tasks in progress are limited to around 3 or 4 tasks in the progress queue, this is also known as a WIP limit. “Controlling the flow of development using the WIP limit will promote continuous development without wasted resources&quot; <span class="citation" data-cites="Nakazawa:2016ip">(Nakazawa and Tanaka <a href="#ref-Nakazawa:2016ip">2016</a>)</span>. This constraint serves the purpose to maintain focus on the tasks that are being worked on, in this case on a weekly basis. Figure <a href="#fig:trello_2">3.2</a> shows the project’s Kanban board on <em>Trello</em> once the task is completed for the week, it is placed onto the <em>Finished</em> board. A <em>Future Goals</em> board is added if there exists more time to add any features to the artefact. One can rearrange the <em>Todo</em> tasks in terms of priority without affecting the <em>Working on (This Week)</em> tasks this allows higher priority tasks to be flexibly rearranged without affecting the tasks that are currently in progress in the <em>Working on (This Week)</em> board.</p>
<figure title="This project’s Trello board.">
<a id="fig:trello_2"></a>
<img src="./figures/figure_18.svg" /><figcaption><b class="captiontitle">Figure 3.2: </b>The project’s <em>Trello</em> Board with 5 boards.<span data-label="fig:trello_2"></span></figcaption>
</figure>
<p>As an example, producing deep learning visualisations may have higher priority than generating reports (If the visualisations for each optimisation algorithm should be included in the generated report) so they can be switched if need be, once any of the tasks on the <em>Working on (This Week)</em> board is complete, then <em>Visualisations</em> would be next to be worked on. “Along with the changes in the state of the task, the task card is moved on the Kanban board from left to right.&quot; <span class="citation" data-cites="Nakazawa:2016ip">(Nakazawa and Tanaka <a href="#ref-Nakazawa:2016ip">2016</a>)</span>.</p>
<p>For this project, a weekly basis was decided as a time boxing technique just to make sure there is a buffer of time to complete all three tasks in the <em>Working on (This Week)</em> board. As a result, this allows a ongoing flow of tasks each week and new tasks can be added if any requirements are added during the duration of the project.</p>
<h2 id="sec:software-development">Software Development</h2>
<p>For the project to succeed, the process in which the software must be built has to be iterative and flexible as opposed to a step-by-step and rigid method that the ‘waterfall’ approach provided. This is summarised by <span class="citation" data-cites="Shaydulin:2017ty">(Shaydulin and Sybrandt <a href="#ref-Shaydulin:2017ty">2017</a>)</span>, “Waterfall typically requires a long time of requirements gathering and project planning before any code is written&quot; <span class="citation" data-cites="Shaydulin:2017ty">(Shaydulin and Sybrandt <a href="#ref-Shaydulin:2017ty">2017</a>, 5)</span>. It is ideal to consider a flexible approach because requirements within any stage of the software development process are subject to change.</p>
<p>Since machine/deep learning is an iterative process that involves a lot of hyperparameter tweaking, an appropriate agile methodology is regarded. <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>)</span> points out this fact: “Our recommended methodology is to iteratively refine the baseline and test whether each change makes an improvement&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 429)</span>. Based on the requirements of this project, Kanban was chosen on the basis that the process is visual, straightforward and flexible. The Kanban board is the best known feature of this methodology that shows the visual progression of the project. “In this way, the Kanban method visualizes the overall flow of development and the state of the tasks and continues to improve the process.&quot; <span class="citation" data-cites="Nakazawa:2016ip">(Nakazawa and Tanaka <a href="#ref-Nakazawa:2016ip">2016</a>)</span>. It is worth noting that other agile processes like Scrum and <strong>eXtreme Programming</strong> (XP) were considered for this project, both were unsuitable since they focus heavily on the customer through user stories, an attribute that this project does not require. Given that it is possible to combine Scrum and Kanban together (Scrumban) this combination would make the project more complex than it needed to be, and not all the processes would be followed given the time constraints of the project.</p>
<h2 id="sec:research-methods">Research Methods</h2>
<p>Since the project’s aim is to measure how well the computer is able to identify an emotion, our method of research is through experimentation. The reason given for this choice is that <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>)</span> argues that there is no ‘best’ machine learning algorithm, and instead “...our goal is to understand what kinds of distributions are relevant to the “...real world“ that an AI agent experiences, and what kinds of machine learning algorithms perform well on data drawn from the kinds of data generating distributions we care about.”<span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 115)</span></p>
<p>Once the emotions for each character is defined into a respective dataset with their labels, different training algorithms are tested at each trial run of the learning process. The algorithms that will be tested will be discussed further in the evaluation stage. For the dataset to be fair, it is best practice to split the dataset into <em>Training</em> and <em>Testing</em> datasets. The training dataset is used to teach the algorithm the correct label upon classification, the testing dataset is used to test if the algorithm has generalised well to the dataset and can be used to test data it has not seen before. It is worth pointing out that this is a crucial practice of machine learning, that no data from the test dataset should <em>ever</em> be included in training. To avoid this, the data is explicitly separated into training and testing folders, for this project the data will be split 80% training, 20% testing.</p>
<p>The experiment begins with a small dataset of emotions to see if the algorithm learns the emotions from the labeled data well. The algorithm that determines this is the cross-entropy loss function. In short, this loss function computes the distance between a predicted label that the computer has learned with the true label. Our goal is to minimise the loss function such that the probability of the predicted and true label are as close to each other as possible. To make sure that this is the case, a common method to tweak the network to get the best results is known as hyperparameter tuning. Since we are using a neural network, the amount of hyperparameter tuning can be very lengthy, so the amount of layers will be small then will gradually add more until the algorithm performs worse on the newly tuned parameters.</p>
<p>Finally, the measurements that indicates how well each algorithm learned the emotions from the dataset is the “model loss&quot;. Which indicates how close the neural network has converged towards representing the dataset. The aim for this metric is that it should be minimised or be comparable to the training set’s performance, and the “model accuracy&quot; which indicates how accurate the neural network is at classifying the dataset, presumably the test set. The aim for this metric is that it must be maximised or be comparable to the training set’s performance.</p>
<h1 id="sec:chap:implementation">Implementation</h1>
<p>This chapter covers the software development lifecycle of the project. Throughout the duration of this project, the required phases have been followed from start to completion.</p>
<h2 id="sec:requirements">Requirements</h2>
<p>Before a dataset of emotion categories can be constructed, key two requirements must be met in order to progress onto the next stage, a suitable deep neural network must be selected to learn emotions on and an appropriate cartoon must be chosen to construct a dataset from. This stage was the first phase of this project.</p>
<h3 id="sec:choice-of-deep-neural-network">Choice of Deep Neural Network</h3>
<p>The choice of neural network was down to how relevant it is towards the aim, which is <em>to measure how accurate a computer can correctly identify an emotion from a given set of images from a cartoon video</em>. that being said images are going to be worked on on the dataset. The neural networks in consideration are the CNN and the RNN. In short, the comparison of the two are that the CNN models over spatial data and the RNN models over temporal data. It is no surprise that the CNN as previously discussed in <a href="#sec:chap:background">Chapter 2</a>, has proven to be successful in image related tasks and would be a excellent choice for the project over the RNN. Having considered even a amalgamation of the two CNN-RNN this would be even more effective on video related tasks.</p>
<p>However, it would slightly deviate from the original aim, to recognise an emotion from a set of images. Given this aim, choosing the CNN for the project would be more appropriate.</p>
<h3 id="sec:choice-of-animated-cartoon">Choice of Animated Cartoon</h3>
<p>There is a clear distinction between what type of cartoon the project needed to recognise emotions from. It was without a doubt it needed to be “animated&quot; since a lot of frames can be extracted and therefore processed later on. The animated cartoon also needed to show varying emotions throughout each episode so that there is enough data in the dataset for the emotions that need to be classified.</p>
<h3 id="sec:tom-jerry"><em>Tom &amp; Jerry</em></h3>
<p><em>Tom &amp; Jerry</em> is an animated cartoon series created by Hanna-Barbera under the production for <strong>Metro-Goldwyn-Mayer</strong> (MGM). The main characters of the cartoon consists of two characters that occur frequently in the series, <em>Tom</em> &amp; <em>Jerry</em>. Tom is the cat that usually goes after Jerry, the mouse in the cartoon that is being chased by Tom. Both of these characters can be argued to be the protagonists in the series. It is composed of 164 episodes averaging around 6 to 10 minutes per episode. Figure <a href="#fig:tom_and_jerry">4.1</a> including both the two characters in the episode <em>‘Fit to be Tied’ (1952)</em>.</p>
<figure title="A scene from Tom & Jerry.">
<a id="fig:tom_and_jerry"></a>
<img src="./figures/figure_19.svg" /><figcaption><b class="captiontitle">Figure 4.1: </b>A scene from the animated cartoon <em>Tom &amp; Jerry</em> Episode 69 - <em>‘Fit to be Tied’ (1952)</em>. © Warner Bros. Entertainment, Inc<span data-label="fig:tom_and_jerry"></span></figcaption>
</figure>
<p><em>Tom &amp; Jerry</em> is an appropriate cartoon for this project because not only there are lots of episodes to build a large dataset from, but because both of the two main characters express various emotions in each of these episodes that can be selected and separated into their respective categories. Since both of the characters are not human, it would be interesting to find out whether the computer could recognise both of these characters emotions, even though their facial structure does not resemble that of humans, which have been proven to been detected in recent years.</p>
<h3 id="sec:dataset-gathering">Dataset Gathering</h3>
<p>For this project, 64 episodes of <em>Tom &amp; Jerry</em> were collected and processed on to create this dataset. A pre-made cartoon dataset was taken into consideration as an alternative, but was not taken further because a there did not exist a pre-made cartoon dataset of faces. In light of this, a dataset had to be created from scratch.</p>
<figure title="YouTube results for the query ‘Tom & Jerry’.">
<a id="fig:tom_and_jerry_dataset_gathering_1"></a>
<img src="./figures/figure_21.svg" /><figcaption><b class="captiontitle">Figure 4.2: </b>Results from the search query <code>Tom &amp; Jerry</code> on <em>YouTube</em>. © Warner Bros. Entertainment, Inc / Jonni Valentayn <span data-label="fig:tom_and_jerry_dataset_gathering_1"></span></figcaption>
</figure>
<p>Videos were collected and downloaded from the online video service <em>YouTube</em>. <em>YouTube</em> was chosen because it is the largest video service and search engine in the world after <em>Google</em>, it was very likely that some <em>Tom &amp; Jerry</em> episodes would be uploaded online. From the channel <em>Joni Valentayn</em>. Of the 164 episodes of the series as of time of writing, only 99 videos were available on the channel. Since <em>Tom &amp; Jerry</em> is indeed copyrighted, the length of each <em>Tom &amp; Jerry</em> episode from YouTube is reduced by a almost a half. Figure <a href="#fig:tom_and_jerry_dataset_gathering_1">4.2</a> shows the search results for the query <code>Tom &amp; Jerry</code> in the dataset gathering process and Figure <a href="#fig:tom_and_jerry_dataset_gathering_2">4.3</a> shows a sample set of videos from the <em>Joni Valentayn</em> channel online. The videos are roughly 3 minutes long, enough for a dataset of emotions for each episode.</p>
<p>The videos were downloaded using a <em>YouTube</em> downloading tool called <code>youtube-dl</code> in the MP4 format. The command used to download each video from the channel was:</p>
<p style="text-align:center"><code>youtube-dl [YOUTUBE-VIDEO-ID]</code></p>
<p>Where <code>[YOUTUBE-VIDEO-ID]</code> is the video’s identifier on <em>YouTube</em>. The videos were downloaded to a folder for further processing. Out of the 99 videos on the channel, 64 Tom &amp; Jerry videos were selected based on varying degrees of emotions in each episode for each character.</p>
<figure title="Sample set of videos from the Joni Valentayn YouTube channel.">
<a id="fig:tom_and_jerry_dataset_gathering_2"></a>
<img src="./figures/figure_20.svg" /><figcaption><b class="captiontitle">Figure 4.3: </b>Sample set of videos from the <em>YouTube</em> channel <em>Joni Valentayn</em>. The videos on this channel are roughly 3 minutes long in duration. © Warner Bros. Entertainment, Inc / Jonni Valentayn<span data-label="fig:tom_and_jerry_dataset_gathering_2"></span></figcaption>
</figure>
<p>The full list of videos that have been used in the creation of this dataset is available in Table <a href="#tab:dataset_videos_1">B.1</a> and Table <a href="#tab:dataset_videos_2">B.2</a> of Appendix <a href="#sec:list-of-episodes-in-dataset">B</a>.</p>
<h3 id="sec:face-segmentation">Face Segmentation</h3>
<p>The best way to segment the cartoon faces from the collection of videos is by using Haar-like features to detect if a face is present, cropping and saving the cropped face to disk for each episode.</p>
<h3 id="sec:haar-like-features">Haar-like features</h3>
<p>Haar-like features have been used to detect objects in videos and images, but their primary and most used application is in detecting faces. They work by examining selected regions of an image, performing a summation on the pixels within one region of the image (white region), then subtracting the sum of the white region with another summed region of the image (black region). These special regions are key in detecting different features in images. Moreover, they come in different forms depending on what feature to detect. Figure <a href="#fig:haar_like">4.4</a> shows the various features that can be detected in an image.</p>
<figure title="Different Haar-like features.">
<a id="fig:haar_like"></a>
<img src="./figures/figure_22.svg" /><figcaption><b class="captiontitle">Figure 4.4: </b>Different Haar-like features that can be used to detect features in an image. The white rectangle is summed and subtracted against the summed result in the black rectangle.<span data-label="fig:haar_like"></span></figcaption>
</figure>
<p>The most common of algorithms for detecting faces is the <em>Viola-Jones Face detection algorithm</em>, which originally introduced the Haar-like features technique. Detectors slide over the target image such as the image in Figure <a href="#fig:viola_jones">4.5</a>, The lines detect the nose, forehead and lighter regions. The rectangles detect the eyes and darker regions of the image. <span class="citation" data-cites="Viola:2001ks">(Viola and Jones <a href="#ref-Viola:2001ks">2001</a>)</span> proposed another algorithm to accompany their findings for Haar-like features, that is the integral image, which makes detecting Haar-like features more efficient. “The integral image can be computed from an image using a few operations per pixel. Once computed, any one of these Harr-like features can be computed at any scale or location in constant time&quot; <span class="citation" data-cites="Viola:2001ks">(Viola and Jones <a href="#ref-Viola:2001ks">2001</a>, 1)</span>.</p>
<figure title="Haar-like features detecting features in a face.">
<a id="fig:viola_jones"></a>
<img src="./figures/figure_23.svg" /><figcaption><b class="captiontitle">Figure 4.5: </b>Example of Haar-like features detecting features in a face in an image.<span data-label="fig:viola_jones"></span></figcaption>
</figure>
<p>However, during the development of this project it is very challenging to detect faces in cartoons. Many of the research in face detection is geared towards human faces, meaning that a face detector trained on detecting human faces would not be able to detect cartoon faces. Haar cascade training is a method that aims to detect any object from a video or an image by using a defined set of positive and negative sample images. The algorithm is similar to how the original Viola-Jones algorithm works, in that both of these algorithms use a machine learning technique called <em>Boosting</em> which aims to combine multiple weak classifiers into a strong classifier. <em>Adaboost</em> (Adaptive Boosting) is used among both of these family of ensemble learning algorithms: “AdaBoost provides an effective learning algorithm and strong bounds on generalization performance&quot; <span class="citation" data-cites="Viola:2001ks">(Viola and Jones <a href="#ref-Viola:2001ks">2001</a>, 2)</span>. The idea behind Adaboost in the context of the Viola-Jones algorithm is to “...select the single rectangle feature which best separates the positive and negative examples&quot; <span class="citation" data-cites="Viola:2001ks">(Viola and Jones <a href="#ref-Viola:2001ks">2001</a>, 3)</span>. This technique can be effectively transferred to train on any image.</p>
<p>There exists pre-trained and ready to use Haar cascades online that can be used without re-training, but the ones that currently exist online can only detect human features, like eyes, lips, mouth and face. Therefore, a custom Haar cascade file needed to be trained on cartoon faces first before it can automatically segment faces in <em>Tom &amp; Jerry</em>. Both the characters Tom and Jerry were chosen to have custom Haar cascades, this will be explained further in the implementation stage.</p>
<h2 id="sec:design">Design</h2>
<p>The design stage was the second phase of the project which dealt with the selection of which emotions to classify, plus the architecture selection and parameters of the CNN, in addition to an overview of how the project is designed.</p>
<h3 id="sec:choice-of-emotions">Choice of emotions</h3>
<p>Due to time constraints of segmenting every emotion from the 6 basic emotions, It was decided that the amount of emotions to be detected in the project had to be halved; only to contain: <em>happy</em>, <em>angry</em> and <em>surprise</em>. This did lower the overall size of the dataset now that the emotions to be classified are only three.</p>
<h3 id="sec:design-of-the-artefact">Design of the artefact</h3>
<p>The process of classifying the 3 chosen emotions is a two step process. The first process shown in Figure <a href="#fig:design_1">4.6</a> uses the dataset collected from <em>YouTube</em> alongside both Tom &amp; Jerry’s custom Haar cascades, are segmented and cropped for each episode automatically. The images are manually annotated afterwards. This is done by placing a given segmented image of say <em>happy</em> into a folder that corresponds with that emotion for each character. The result is 3 folders with the emotions <em>happy</em>, <em>angry</em> and <em>surprise</em> for each character.</p>
<figure title="The architecture of the 1st step process, dataset construction & segmentation.">
<a id="fig:design_1"></a>
<img src="./figures/figure_24.svg" /><figcaption><b class="captiontitle">Figure 4.6: </b>The architecture of the step process, dataset construction &amp; segmentation. © Warner Bros. Entertainment, Inc<span data-label="fig:design_1"></span></figcaption>
</figure>
<p>The second part of the two step process shown in Figure <a href="#fig:design_2">4.7</a> was the classification process. From the newly created dataset of segmented images, the idea is to have 400 in each category. The images for each emotion are labelled using a sparse encoding scheme called 1-hot encoding. This encoding ensures that a sample image from the dataset is correctly annotated to one of the three emotions. “If <span class="math inline">\(x\)</span> belongs to [category] i, then <span class="math inline">\(h_i\)</span> = 1 and all other entries of the representation <span class="math inline">\(h\)</span> are zero&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 146)</span>. The datasets for each emotion are labelled and are passed to the CNN which produces a softmax prediction of a given test image.</p>
<figure title="The architecture of the 2nd step process.">
<a id="fig:design_2"></a>
<img src="./figures/figure_25.svg" /><figcaption><b class="captiontitle">Figure 4.7: </b>The architecture of the step process, classification. © Warner Bros. Entertainment, Inc<span data-label="fig:design_2"></span></figcaption>
</figure>
<h3 id="sec:design-of-the-convolutional-neural-network">Design of the Convolutional Neural Network</h3>
<p>The design of the CNN was based on how much data that was collected for each emotion. The architecture did not need to be too big otherwise it would overfit the data, too small and it would certainly underfit. It was decided to use either a 3 to 5 layer CNN architecture and ultimately choose the performant model. When the dataset is passed into the CNN network as an input, the images are resized to 60<span class="math inline">\(\times\)</span>60 pixels with 3 channels (RGB) resulting in a final size of <strong>60<span class="math inline">\(\times\)</span>60<span class="math inline">\(\times\)</span>3</strong>. Dropout is applied at the end of the convolutions and max pooling; as a regularisation procedure to remove neurons and the network. “One advantage of dropout is that it is very computationally cheap&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 257)</span>. The neurons are then flattened in preparation to become passed into the two fully connected layers. The first fully connected layer has 512 neurons with ReLU activation, and the last fully connected layer, (the output layer) has 6 neurons which are our emotions.</p>
<p>Since this project only focuses on only 3 emotions, it is expected that the output for the rest of the last three excluded emotions: <em>sad</em>, <em>fear</em> and <em>disgust</em> would be zero. Despite this, the first 3 of the 6 neurons will represent <em>happy</em>, <em>angry</em> and <em>surprise</em>. Due to our target image having the possibility of being either one of the three class labels, the softmax activation function is used for the last layer and cross entropy is used as the cost function. “The use of cross-entropy losses greatly improved the performance of models with sigmoid and softmax outputs&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 219)</span>.</p>
<h2 id="sec:development">Development</h2>
<p>This stage covers the software development and the tools used when building the artefact. This was the third phase of the project.</p>
<h3 id="sec:tools">Tools</h3>
<p>The artefact was made using several tools that made the development quicker in terms of time and testing. The tools used in this project are outlined below with an explanation of it’s effectiveness and adoption by community and industry.</p>
<h4 id="sec:python">Python</h4>
<p>Python is an open source, general purpose programming language originally developed by Guido van Rossum (currently developed by the <strong>Python Software Foundation</strong> (PSF)) designed to be fast and expressive to both read and write in for experts and beginners. Readability is not only the main advantage that Python has over other programming languages, It has been widely embraced by the scientific community, often favoured over traditional programming languages such as C++, Fortran and Java. <span class="citation" data-cites="Perez:2011tp">Pérez, Granger, and Hunter (<a href="#ref-Perez:2011tp">2011</a>)</span> argues that an interactive environment is more suitable for scientists in terms of flexibility. For Python, it delivers immediate feedback when executing code, and this property is hard to express in conventional languages. <span class="citation" data-cites="Perez:2011tp">(<a href="#ref-Perez:2011tp">2011</a>, 14)</span>.</p>
<p>With any programming language, the community support and software libraries play a factor in it’s adoption. Python has a vast amount of scientific software libraries that are available online; most of which are open source and can work well with other languages such as C++. “...it’s particularly good at interoperating with multiple languages&quot; structures and with a syntax accessible to scientists who aren’t programmers. <span class="citation" data-cites="Perez:2011tp">(Pérez, Granger, and Hunter <a href="#ref-Perez:2011tp">2011</a>, 14)</span>.</p>
<p>In the context of building the artefact, experimentation is quicker and easier in Python such that the prototype of the artefact can be developed and even moved onto another language. On the other hand it can also be argued that a solution can be entirely built in Python and that the scientific community can benefit since the code is readable and can be reproduced easily. In short, “Python combines high-level flexibility, readability, and a well defined interface with low-level capabilities&quot; <span class="citation" data-cites="Perez:2011tp">(Pérez, Granger, and Hunter <a href="#ref-Perez:2011tp">2011</a>, 15)</span>.</p>
<h4 id="sec:opencv">OpenCV</h4>
<p>OpenCV is an open source computer vision library developed in C/C++. Originally developed by Intel (currently developed by Itseez) that offers high performance media manipulation and object detection algorithms for applications ranging in image and video processing, robotics and mobile applications.</p>
<p>The use of OpenCV is common for face detection and recognition applications and the library provides functionality for this. <span class="citation" data-cites="Jalled:2016vi">(Jalled and Voronkov <a href="#ref-Jalled:2016vi">2016</a>)</span> used the library for detecting faces using an <strong>Unmanned Aerial Vehicle</strong> (UAV). The usefulness of OpenCV is that is offers bindings to other languages, which means that ‘foreign’ languages such as Python, Ruby, Java and Perl can take advantage of OpenCV without reinventing the wheel. The UAV face detection solution by <span class="citation" data-cites="Jalled:2016vi">Jalled and Voronkov (<a href="#ref-Jalled:2016vi">2016</a>)</span> was developed in Python and OpenCV (with Python bindings) and prefers Python over the alternative, MATLAB because Python execution time is smaller than MATLAB and is more simpler <span class="citation" data-cites="Jalled:2016vi">(<a href="#ref-Jalled:2016vi">2016</a>)</span>.</p>
<p>This artefact uses the OpenCV library in the dataset construction &amp; segmentation stage. The creation of the custom Haar cascades for both Tom and Jerry was made using OpenCV. Specifically the command line tools: <code>opencv_createsamples</code> &amp; <code>opencv_traincascade</code>. The first tool creates a training set of positive samples, the negative samples contains anything <strong>that is not in the positive samples</strong> since including any would produce a higher rate of misclassification when attempting to recognise the cartoon faces. The second tool trains the classifier and generates the Haar cascade as an XML file to use within the OpenCV library.</p>
<figure title="Haar cascade training for positive images.">
<a id="fig:haar_positive_images"></a>
<p><img src="./figures/figure_26.svg" title="Haar cascade training for positive images." /></p>
<p><img src="./figures/figure_27.svg" title="Haar cascade training for positive images." /></p>
<figcaption><b class="captiontitle">Figure 4.8: </b>Haar cascade training for positive images. The training process requires this separation
of positive/negative to ensure that the face detector can determine between both characters faces
for image segmentation.
(Left) Tom (Right) Jerry
© Warner Bros. Entertainment, Inc<span data-label="fig:haar_positive_images"></span></figcaption>
</figure>
<h4 id="sec:keras">Keras</h4>
<p>Keras is a Python deep learning library which was developed by François Chollet, with the intention of facilitating quick and rapid experimentation of building neural networks. Keras builds on top of two existing machine learning libraries Theano and Tensorflow.</p>
<p>Although Theano is older than Tensorflow, Tensorflow has grown to be one of the most popular machine/deep learning libaries and is supported in by Google, the creators of the project. It is also similar to Theano. According to <span class="citation" data-cites="Abadi:2016vn">(Abadi et al. <a href="#ref-Abadi:2016vn">2016</a>)</span> Both Theano and Tensorflow have a ‘data-flow graph’: “TensorFlow uses a unified dataflow graph to represent both the computation in an algorithm and the state on which the algorithm operates&quot; <span class="citation" data-cites="Abadi:2016vn">(Abadi et al. <a href="#ref-Abadi:2016vn">2016</a>, 1)</span> and goes on to further state: “...[Theano’s] programming model is closest to TensorFlow, and it provides much of the same flexibility in a single machine&quot;<span class="citation" data-cites="Abadi:2016vn">(Abadi et al. <a href="#ref-Abadi:2016vn">2016</a>, 2)</span>.</p>
<p>Both of these libraries are powerful and are flexible compared to Keras, However, both have their disadvantages, Theano and Tensorflow are not modular in comparison to Keras. Meaning they are not suitable for prototyping neural networks. Defining a neural network in Keras is straightforward if not intuitive compared to Tensorflow/Theano.</p>
<p>Since Keras is built on top of both Tensorflow and Theano, the choice of which ‘backend’ library to use is up to the user. The artefact uses the Tensorflow backend since Keras has officially supported it. The version of Keras that the artefact uses is 2.0. This compatibility with Tensorflow is important, because it has more of a chance of researchers being able to reproduce results when running the code on a different machine without errors.</p>
<h3 id="sec:cartoon-face-segmentation">Cartoon Face Segmentation</h3>
<p>The cartoon face segmentation tool is implemented in Python and the code is shown in Appendix <a href="#sec:cartoon-face-segmentation-1">A</a>. Put simply, for every character in the dataset, the tool processes an episode and reads each frame from the video. For every frame, the tool tries to detect a cartoon face using a custom Haar cascade file loaded into the program; created for both Tom and Jerry to be detected in the frame. Listing <a href="#listing:haar_cascade">1</a> shows the code responsible for detecting faces in a frame.</p>
<div style="background: #FFFFFF; overflow:auto;width:auto;gray;border-width:.0em .1em .1em .8em;padding:.2em .6em;">
<a id="listing:haar_cascade"></a>    
<table><tr><td><pre style="margin: 0; line-height: 125%">39
40
41
42
43
44
45
46
47
48
49
50
51
52
53</pre></td><td><pre style="margin: 0; line-height: 125%"><span style="color: #008000; font-weight: bold">if</span> character[<span style="color: #BA2121">&#39;name&#39;</span>] <span style="color: #666666">==</span> <span style="color: #BA2121">&quot;Tom&quot;</span>:
            <span style="color: #408080; font-style: italic"># detect faces in our image.</span>
            faces <span style="color: #666666">=</span> face_cascade<span style="color: #666666">.</span>detectMultiScale(frame, 
                        scaleFactor<span style="color: #666666">=1.10</span>, 
                        minNeighbors<span style="color: #666666">=40</span>, 
                        minSize<span style="color: #666666">=</span>(<span style="color: #666666">24</span>, <span style="color: #666666">24</span>), 
                        flags<span style="color: #666666">=</span>cv2<span style="color: #666666">.</span>cv<span style="color: #666666">.</span>CV_HAAR_SCALE_IMAGE
            )
        <span style="color: #008000; font-weight: bold">else</span>:
            faces <span style="color: #666666">=</span> face_cascade<span style="color: #666666">.</span>detectMultiScale(frame, 
                        scaleFactor<span style="color: #666666">=1.10</span>, 
                        minNeighbors<span style="color: #666666">=20</span>, 
                        minSize<span style="color: #666666">=</span>(<span style="color: #666666">24</span>, <span style="color: #666666">24</span>), 
                        flags<span style="color: #666666">=</span>cv2<span style="color: #666666">.</span>cv<span style="color: #666666">.</span>CV_HAAR_SCALE_IMAGE
            )
</pre></td></tr></table>
<figcaption align="center"><b class="captiontitle">Listing 1: </b>Haar cascade detector code for Tom & Jerry. The <code>minNeighbors</code> parameter controls the minimum neighbours only different for Jerry because he has a smaller face than Tom.<span data-label="listing:haar_cascade"></span></figcaption>
</div>


<figure title="Before and after segmenting a region of a face">
<a id="fig:segmentation_1"></a>
<img src="./figures/figure_28.svg" /><figcaption><b class="captiontitle">Figure 4.9: </b>Result before and after segmenting a region of a face from one frame of a video. © Warner Bros. Entertainment, Inc<span data-label="fig:segmentation_1"></span></figcaption>
</figure>
<p>  Figure <a href="#fig:segmentation_1">4.9</a> shows the segmentation process for 1 frame in an episode and Figure <a href="#fig:haar_positive_images">4.8</a> shows the Haar cascade training for previously segmented images. Sometimes other characters are detected in the video and are also segmented and saved into the dataset. This is where after the tool has finished processing one episode, the dataset has to be cleaned. This process just removes images that are not Tom or Jerry. For the labelling process however, a further dataset cleaning has to be done to properly separate Tom and Jerry’s emotions into different folders.</p>
<h3 id="sec:tom-jerry-image-dataset"><em>Tom &amp; Jerry</em> Image Dataset</h3>
<p>After segmenting <strong>159,035</strong> images (593.5MB) (including Haar cascade positive images) from 64 episodes of <em>Tom &amp; Jerry</em>, the total number of images segmented in the unlabelled dataset is <strong>141,893</strong> images (515.8MB). The dataset was further reduced to only 3 emotions, by selecting <strong>400</strong> training and test images in each emotion category, meaning <strong>800</strong> images (incl. test images) for each emotion. In total for 3 emotions of Tom &amp; Jerry, the final dataset contains <strong>4,800</strong> images, 15MB in size.</p>
<h3 id="sec:training-classification-and-visualisation">Training, Classification and Visualisation</h3>
<p>The training/classification/visualisation tool is implemented in Python and the code is shown in Appendix <a href="#sec:classification">C</a>. The training process begins by loading <strong>1,200</strong> images for both training and testing images for Tom and Jerry. In addition, their labels are also loaded and encoded in a 1-hot encoding scheme. Figure <a href="#fig:design_2">4.7</a> shows an example of the scheme in the ‘1-Hot Encoding’ section. The model of the CNN is loaded and trained by fitting the training set and labels to the test set and labels. The code for the CNN architecture described in the design stage is shown in Listing <a href="#listing:cnn_model">2</a>. The code on Line 327 are the tested optimisers will be discussed in the evaluation stage. After training, the learned weights are saved into a model file. Keras uses <strong>Hierarchical Data Format 5</strong> (HDF5) (<code>.h5</code>) as the storage format to store it’s models by default.</p>
<div style="background: #FFFFFF; overflow:auto;width:auto;gray;border-width:.0em .1em .1em .8em;padding:.2em .6em;">
<a id="listing:cnn_model"></a>     
<table><tr><td><pre style="margin: 0; line-height: 175%">
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339</pre></td><td><pre style="margin: 0; line-height: 125%"><span style="color: #408080; font-style: italic"># the main convolutional neural network architecture.</span>
<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">load_cnn_model</span>():
    <span style="color: #408080; font-style: italic"># define convnet model.</span>
    cnn <span style="color: #666666">=</span> Sequential()

    <span style="color: #408080; font-style: italic"># 3x3 convolution &amp; 2x2 maxpooling with a input image of 60x60x3.</span>
    cnn<span style="color: #666666">.</span>add(Conv2D(<span style="color: #666666">32</span>, (<span style="color: #666666">3</span>, <span style="color: #666666">3</span>), activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;relu&#39;</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&#39;same&#39;</span>, input_shape<span style="color: #666666">=</span>(<span style="color: #666666">3</span>, w, h), name<span style="color: #666666">=</span><span style="color: #BA2121">&quot;conv_layer_1&quot;</span>))
    cnn<span style="color: #666666">.</span>add(MaxPooling2D(pool_size<span style="color: #666666">=</span>(<span style="color: #666666">2</span>, <span style="color: #666666">2</span>), name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;maxpool_1&#39;</span>))

    <span style="color: #408080; font-style: italic"># 3x3 convolution &amp; 2x2 maxpooling.</span>
    cnn<span style="color: #666666">.</span>add(Conv2D(<span style="color: #666666">32</span>, (<span style="color: #666666">3</span>, <span style="color: #666666">3</span>), activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;relu&#39;</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&#39;same&#39;</span>, name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;conv_layer_2&#39;</span>))
    cnn<span style="color: #666666">.</span>add(MaxPooling2D(pool_size<span style="color: #666666">=</span>(<span style="color: #666666">2</span>, <span style="color: #666666">2</span>), name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;maxpool_2&#39;</span>))

    <span style="color: #408080; font-style: italic"># 3x3 convolution &amp; 9x9 maxpooling.</span>
    cnn<span style="color: #666666">.</span>add(Conv2D(<span style="color: #666666">32</span>, (<span style="color: #666666">3</span>, <span style="color: #666666">3</span>), activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;relu&#39;</span>, padding<span style="color: #666666">=</span><span style="color: #BA2121">&#39;same&#39;</span>, name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;conv_layer_3&#39;</span>))
    cnn<span style="color: #666666">.</span>add(MaxPooling2D(pool_size<span style="color: #666666">=</span>(<span style="color: #666666">9</span>, <span style="color: #666666">9</span>), name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;maxpool_3&#39;</span>))

    <span style="color: #408080; font-style: italic"># dropout 50% and flatten layer.</span>
    cnn<span style="color: #666666">.</span>add(Dropout(<span style="color: #666666">0.5</span>))
    cnn<span style="color: #666666">.</span>add(Flatten(name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;flatten_1&#39;</span>))

    <span style="color: #408080; font-style: italic"># fully connected layers and the output layer.</span>
    cnn<span style="color: #666666">.</span>add(Dense(<span style="color: #666666">512</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;relu&#39;</span>, name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;fully_connected_1&#39;</span>))
    cnn<span style="color: #666666">.</span>add(Dense(<span style="color: #666666">6</span>, activation<span style="color: #666666">=</span><span style="color: #BA2121">&#39;softmax&#39;</span>, name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;output_layer&#39;</span>))
    o <span style="color: #666666">=</span> optimizers<span style="color: #666666">.</span>SGD(lr<span style="color: #666666">=0.01</span>, decay<span style="color: #666666">=1e-6</span>, momentum<span style="color: #666666">=0.9</span>, nesterov<span style="color: #666666">=</span><span style="color: #008000">True</span>)
    cnn<span style="color: #666666">.</span>compile(loss<span style="color: #666666">=</span><span style="color: #BA2121">&#39;categorical_crossentropy&#39;</span>, optimizer<span style="color: #666666">=</span>o, metrics<span style="color: #666666">=</span>[<span style="color: #BA2121">&#39;accuracy&#39;</span>])

    <span style="color: #408080; font-style: italic"># return the cnn model.</span>
    <span style="color: #008000; font-weight: bold">return</span> cnn
</pre></td></tr></table>
<figcaption align="center"><b class="captiontitle">Listing 2: </b>The designed CNN model in Python.<span data-label="listing:cnn_model"></span></figcaption>
</div>


<p>The tool can also perform emotion classification, although this happens almost immediately after the training process, the tool can classify any image as long as the trained weights exist. Either way, the tool loads the learned weights from the model file. Listing <a href="#listing:cnn_model">3</a> shows the predictions for the emotions.</p>

<div style="background: #FFFFFF; overflow:auto;width:auto;gray;border-width:.0em .1em .1em .8em;padding:.2em .6em;">
<a id="listing:cnn_model_probabilities"></a>     
<table><tr><td><pre style="margin: 0; line-height: 130%">353
354
355
356</pre></td><td><pre style="margin: 0; line-height: 125%"><span style="color: #408080; font-style: italic"># get the predicted class and the predicted probabilities.</span>
        pred_class, prob <span style="color: #666666">=</span> (cnn<span style="color: #666666">.</span>predict_classes(loaded_img, verbose<span style="color: #666666">=0</span>)[<span style="color: #666666">0</span>], cnn<span style="color: #666666">.</span>predict(loaded_img, verbose<span style="color: #666666">=0</span>)<span style="color: #666666">.</span>flatten())
        predicted_emotion <span style="color: #666666">=</span> <span style="color: #008000">str</span>(emotions[pred_class])
        confidence_score <span style="color: #666666">=</span> <span style="color: #008000">float</span>(prob[pred_class] <span style="color: #666666">*</span> <span style="color: #666666">100</span>)
</pre></td></tr></table>
<figcaption align="center"><b class="captiontitle">Listing 3: </b>The code that predicts emotions from a randomly chosen image <i>I</i> in the test dataset.<span data-label="listing:cnn_model_probabilities"></span></figcaption>
</div>


<p>  Line 288 returns the emotion prediction label as an array index, plus the probability of all emotions, which is the output of the last layer of the network. Line 290 returns the predicted emotion label, that uses the <code>np.argmax(x)</code> function. It takes an array and returns the array index containing the maximum value. Thankfully, the array that Line 290 takes in is one-hot encoded, that means that the array with a ‘1’ inside is the array index of the emotion. For example: <code>np.argmax([0,0,1])</code> would return array index ‘2’. Finally the score is converted to a percentage, and the emotion class label prediction is later displayed on the screen. Figure <a href="#fig:example_results">4.10</a> shows the output of the emotion classification, correct predictions are in green and incorrect predictions are in red.</p>
<figure title="Result after multi-class classification.">
<a id="fig:example_results"></a>
<img src="./figures/figure_29.svg" /><figcaption><b class="captiontitle">Figure 4.10: </b>Result after multi-class classification. © Warner Bros. Entertainment, Inc<span data-label="fig:example_results"></span></figcaption>
</figure>
<p>    Finally, the tool can produce convolution layer visualisations to better understand what features the CNN is learning. In Figure <a href="#fig:cnn_visualisation">4.11</a> these visualisations were generated after 50 epochs of training, for 3 layers of the CNN. It is observed that the deeper convolution layers in network, the more filters get visualised. In the first visualisation there are many filters that are blank such as filter 0, 3 and 6. On the other hand, the second convolution layer shows more distinct features and patterns such as colour lines and grainy dots, it is more uniform than the first convolution visualisation. The visualisation of the output layer seems to be learning colours to associate with an emotion although more epochs in training may make it more clearer.</p>
<figure title="Convolution visualisations.">
<a id="fig:cnn_visualisation"></a>
<img src="./figures/figure_30.svg" /><figcaption><b class="captiontitle">Figure 4.11: </b>Convolution visualisations of the first, second and output layers.<span data-label="fig:cnn_visualisation"></span></figcaption>
</figure>
<h1 id="sec:chap:testing_evaluation">Testing &amp; Evaluation</h1>
<p>This chapter covers the evaluation stage where the artefact was tested against 5 optimisation algorithms to see which algorithm better fits the model. Other hyperparameters have been changed in the original artefact to find out if learning and generalisation would improve.</p>
<h2 id="sec:preparation">Preparation</h2>
<p>As mentioned in the design stage, the dataset had to be split into 80% training and 20% testing. This partition is necessary because it shows how well the algorithm performs on the partitioned dataset rather than the whole. It tells us whether our algorithm is underfitting or overfitting without loading the entire dataset. It is common for the partition to be 70:30 or 80:20. Only the 80:20 split has been tested due to time constraints, although it was originally planned for this stage.</p>
<p>It is also a best practice to shuffle the dataset after it has been split. “In cases such as these where the order of the dataset holds some significance, it is necessary to shuffle the examples before selecting minibatches&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 271)</span> and further states a warning for not performing this step: “Failing to ever shuffle the examples in any way can seriously reduce the effectiveness of the algorithm.&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 271)</span>. Listing <a href="#listing:shuffle_split">4</a> shows the portion of code that is responsible for this procedure.</p>
<p>   </p>

<div style="background: #FFFFFF; overflow:auto;width:auto;gray;border-width:.0em .1em .1em .8em;padding:.2em .6em;">
<a id="listing:shuffle_split"></a>    
<table><tr><td><pre style="margin: 0; line-height: 125%">222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242</pre></td><td><pre style="margin: 0; line-height: 125%"><span style="color: #408080; font-style: italic"># process testing and training images -&gt; numpy arrays.</span>
    train_images <span style="color: #666666">=</span> process_images(training[<span style="color: #666666">0</span>])
    test_images <span style="color: #666666">=</span> process_images(testing[<span style="color: #666666">0</span>])

    <span style="color: #408080; font-style: italic"># convert training and testing to one hot vectors.</span>
    train_labels <span style="color: #666666">=</span> one_hot(training[<span style="color: #666666">1</span>], num_classes<span style="color: #666666">=6</span>)
    test_labels <span style="color: #666666">=</span> one_hot(testing[<span style="color: #666666">1</span>], num_classes<span style="color: #666666">=6</span>)

    <span style="color: #408080; font-style: italic"># shuffle training data in sync for better training.</span>
    rng <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>get_state()
    np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>shuffle(train_images)
    np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>set_state(rng)
    np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>shuffle(train_labels)

    <span style="color: #408080; font-style: italic"># partition dataset 80/20. (80 -&gt; training, 20 -&gt; testing)</span>
    r <span style="color: #666666">=</span> np<span style="color: #666666">.</span>random<span style="color: #666666">.</span>rand(train_images<span style="color: #666666">.</span>shape[<span style="color: #666666">0</span>])
    part <span style="color: #666666">=</span> r <span style="color: #666666">&lt;</span> np<span style="color: #666666">.</span>percentile(r, <span style="color: #666666">80</span>)
    train_images <span style="color: #666666">=</span> train_images[part]
    train_labels <span style="color: #666666">=</span> train_labels[part]
    test_images <span style="color: #666666">=</span> test_images[<span style="color: #666666">-</span>part]
    test_labels <span style="color: #666666">=</span> test_labels[<span style="color: #666666">-</span>part]
</pre></td></tr></table>
<figcaption align="center"><b class="captiontitle">Listing 4: </b>Code portion that shuffles and partition the dataset..<span data-label="listing:shuffle_split"></span></figcaption>
</div>


<p>It is worth noting that the results produced in this report have been assigned a fixed seed of <code>12379231</code>. This is to ensure that the results in this report can be reproduced.</p>
<h2 id="sec:optimisation-algorithms">Optimisation Algorithms</h2>
<p>The optimisation algorithms are gradient based and are used in conjunction with backpropagation in the CNN. “Gradient descent is a way to minimize an objective function <span class="math inline">\(J(\theta)\)</span> parameterized by a model’s parameters <span class="math inline">\( \theta \in \mathbb{R}^d \)</span> by updating the parameters in the opposite direction of the gradient of the objective function <span class="math inline">\( \nabla_\theta J(\theta) \)</span> [with respect to] the parameters.&quot; <span class="citation" data-cites="Ruder:2016tr">(Ruder <a href="#ref-Ruder:2016tr">2016</a>, 1)</span>. The following optimisation algorithms that are tested and evaluated in the artefact are the following:</p>
<ul>
<li><p><strong>Stochastic Gradient Descent</strong> (SGD)</p></li>
<li><p><strong>Adagrad</strong></p></li>
<li><p><strong>Adadelta</strong></p></li>
<li><p><strong>RMSprop</strong></p></li>
<li><p><strong>Adam</strong></p></li>
</ul>
<h3 id="sec:stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p><strong>Stochastic Gradient Descent</strong> (SGD) is an optimisation algorithm that performs “a parameter update for each training example <span class="math inline">\(x^{(i)}\)</span> and label <span class="math inline">\(y^{(i)}\)</span>.&quot; <span class="citation" data-cites="Ruder:2016tr">(Ruder <a href="#ref-Ruder:2016tr">2016</a>, 2)</span>. In contrast to normal gradient descent methods such as batch gradient descent, that takes longer to converge towards the objective function for a number of steps, would be unsuitable for large datasets, whereas SGD is suitable. The iterative formula for learning rule for SGD is shown in Equation <a href="#eq:5.1">5.1</a></p>
<div>
<a id="eq:5.1"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:12}
    \theta = \theta -− \eta \nabla_\theta J(x(i); y(i); \theta).\]</span></td>
<td style="text-align: right;"><span class="math display">\[(5.1)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>SGD uses a small portion of the training set to estimate the next gradient step, this step must be in the negative gradient of the objective function <span class="math inline">\( J(\theta) \)</span>. In turn, this is argued to converge faster than traditional gradient descent methods. However, the trade off is high variance in it’s estimates, “...that cause the objective function to fluctuate heavily&quot; <span class="citation" data-cites="Ruder:2016tr">(Ruder <a href="#ref-Ruder:2016tr">2016</a>, 2)</span>.</p>
<p>“Slightly modified versions of the stochastic gradient descent algorithm remain the dominant training algorithms for deep learning models today.&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 15)</span> The variants of SGD such as Minibatch Gradient Descent, Momentum, and Nesterov Momentum are used as parameters in the training process and are not discussed due to sake of brevity. Instead, other optimisation algorithms that greatly improve upon SGD, in addition to having the adaptive learning property are discussed below.</p>
<h3 id="sec:adagrad">Adagrad</h3>
<p>Adagrad (Adaptive gradient) is a gradient based optimisation algorithm, that uses a dynamic learning rate “[which] assigns [a] higher learning rate to the parameters that have been updated more mildly and assigns [a] lower learning rate to the parameters that have been updated dramatically.&quot; <span class="citation" data-cites="wang:2017">(Wang, Raj, and Xing <a href="#ref-wang:2017">2017</a>, 54)</span>. Adagrad has an advantage as argued by <span class="citation" data-cites="Ruder:2016tr">Ruder (<a href="#ref-Ruder:2016tr">2016</a>)</span> in that the use of an adaptive learning rate makes manual tuning redundant. This is evident in the gradient update rule in Equation <a href="#eq:5.2">5.2</a>. For each <span class="math inline">\(t\)</span> timestep (epoch), the learning rate <span class="math inline">\(\eta\)</span> is changed by Adagrad and affects every parameter <span class="math inline">\(\theta^t\)</span>. This is based on the previous gradients for <span class="math inline">\(\theta^t\)</span>. <span class="citation" data-cites="Ruder:2016tr">(<a href="#ref-Ruder:2016tr">2016</a>, 6)</span>.</p>
<div>
<a id="eq:5.2"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:13}
    \theta^{t+1} = \theta^{t} - \frac{\eta}{\sqrt{G_{t} + \epsilon}} \odot g_{t}\]</span></td>
<td style="text-align: right;"><span class="math display">\[(5.2)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>However, this comes at a cost, “Adagrad’s main weakness is its accumulation of the squared gradients in the denominator&quot;, and goes on to further state: <span class="citation" data-cites="Ruder:2016tr">(Ruder <a href="#ref-Ruder:2016tr">2016</a>)</span>: “This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge.&quot; <span class="citation" data-cites="Ruder:2016tr">(Ruder <a href="#ref-Ruder:2016tr">2016</a>, 6)</span> This means convergence to a minimum would be very slow for each epoch in the training process. Despite the problem of the excessively slow learning rate, <span class="citation" data-cites="Goodfellow-et-al-2016">Goodfellow, Bengio, and Courville (<a href="#ref-Goodfellow-et-al-2016">2016</a>)</span> argues that Adagrad performs for some deep learning models but not all of them. <span class="citation" data-cites="Goodfellow-et-al-2016">(<a href="#ref-Goodfellow-et-al-2016">2016</a>, 299)</span></p>
<h3 id="sec:adadelta">Adadelta</h3>
<p>Adadelta (Adaptive delta) is another gradient based optimisation algorithm that aims to solve the decreasing learning rate problem from Adagrad. According to <span class="citation" data-cites="Zeiler:2012uw">Zeiler (<a href="#ref-Zeiler:2012uw">2012</a>)</span> accumulation of past gradients is restricted by using a fixed window size <span class="math inline">\(w\)</span> to ensure learning continues after many iterations <span class="citation" data-cites="Zeiler:2012uw">(<a href="#ref-Zeiler:2012uw">2012</a>, 3)</span>. The final update rule for Adadelta is shown in <a href="#eq:5.3">5.3</a>, the changes from the original Adagrad update rule is that the denominator now contains the <strong>Root Mean Square</strong> (RMS) of the gradients and the learning rate is replaced with the RMS error of the update parameters in the previous timestep (epoch) in the numerator.</p>
<div>
<a id="eq:5.3"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:14}
    \Delta \theta_t = - \frac{RMS[\Delta \theta]_{t-1}}{RMS[g]_{t}} g_{t}\]</span></td>
<td style="text-align: right;"><span class="math display">\[(5.3)\]</span></td>
</tr>
</tbody>
</table>
</div>
<div>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:15}
    \theta_{t+1} = \theta_t + \Delta \theta_t\]</span></td>
<td style="text-align: right;"><span class="math display">\[(5.4)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p><span class="citation" data-cites="Ruder:2016tr">Ruder (<a href="#ref-Ruder:2016tr">2016</a>)</span> asserts that the latter replacement removes the dependency of a default learning rate in Adadelta. <span class="citation" data-cites="Ruder:2016tr">(<a href="#ref-Ruder:2016tr">2016</a>, 6)</span></p>
<h3 id="sec:rmsprop">RMSProp</h3>
<p>RMSProp (Root Mean Square Propagation) is a similar gradient based optimisation algorithm to Adadelta, but were independently developed with the same goal as an alternative to Adagrad.</p>
<p>The idea behind RMSProp is to “...[change] the gradient accumulation into an exponentially weighted moving average&quot; <span class="citation" data-cites="Goodfellow-et-al-2016">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>, 299)</span>. The interesting property of the <span class="math inline">\(\rho\)</span> parameter in RMSProp is a hyperparameter according to <span class="citation" data-cites="Goodfellow-et-al-2016">Goodfellow, Bengio, and Courville (<a href="#ref-Goodfellow-et-al-2016">2016</a>)</span> that controls the length scale of the moving average window, and further states that RMSProp is used in practice as one of the recommended optimisers <span class="citation" data-cites="Goodfellow-et-al-2016">(<a href="#ref-Goodfellow-et-al-2016">2016</a>, 301)</span>.</p>
<h3 id="sec:adam">Adam</h3>
<p>From it’s name, ‘Adam’ stands for ‘Adaptive moments’ and is a recent gradient optimisation algorithm that uses two moments, ( and order moments) to compute “adaptive learning rates for different parameters&quot; <span class="citation" data-cites="Kingma:2014us">(Kingma and Ba <a href="#ref-Kingma:2014us">2014</a>, 1)</span>. This is an advantage for Adam because it makes the algorithm more likely to converge quicker.</p>
<p>It is argued that Adam shares resemblance to RMSProp and Adadelta. <span class="citation" data-cites="Ruder:2016tr">(Ruder <a href="#ref-Ruder:2016tr">2016</a>)</span> explains this similarity, “...like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients&quot; <span class="citation" data-cites="Ruder:2016tr">(Ruder <a href="#ref-Ruder:2016tr">2016</a>, 7)</span>. But Adam’s primary difference is that it uses momentum to provide faster minimisation. “The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients&quot; <span class="citation" data-cites="Kingma:2014us">(Kingma and Ba <a href="#ref-Kingma:2014us">2014</a>, 1)</span>. The moments estimated by Adam are known as <span class="math inline">\(m_t\)</span> (moving average gradient) and <span class="math inline">\(v_t\)</span> (squared gradient). Equation <a href="#eq:5.5">5.5</a> shows the update rule for Adam.</p>
<div>
<a id="eq:5.5"></a>
<table style="width:99%;">
<colgroup>
<col style="width: 90%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math display">\[\label{eq:16}
    \theta_{t+1} = \theta_{t} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\]</span></td>
<td style="text-align: right;"><span class="math display">\[(5.5)\]</span></td>
</tr>
</tbody>
</table>
</div>
<p>An important difference and advantage for Adam is the inclusion of a bias correction step. <span class="citation" data-cites="Goodfellow-et-al-2016">Goodfellow, Bengio, and Courville (<a href="#ref-Goodfellow-et-al-2016">2016</a>)</span> mentions that RMSProp with momentum is the closest optimiser to Adam, but lacks bias correction, meaning that RMSProp with momentum may have high bias during training and recommends Adam for being robust to the choice of hyperparameters <span class="citation" data-cites="Goodfellow-et-al-2016">(<a href="#ref-Goodfellow-et-al-2016">2016</a>, 302)</span>.</p>
<h2 id="sec:results">Results</h2>
<p>The dataset was trained on one GPU (Nvidia GeForce GTX 970) with a limit of 50 epochs per test run. In total, 5 test runs were made for each optimiser in which the hyperparameters were changed for each test run.</p>
<p>The hyperparameters that were changed all through the test runs are the following:</p>
<ul>
<li><p><strong>Learning rate</strong></p></li>
<li><p><strong>Max pooling size</strong></p></li>
<li><p><strong>Hidden Layer size</strong></p></li>
<li><p><strong>Dropout percentage</strong></p></li>
</ul>
<p>A hyperparameter and value in <strong>bold</strong> indicates the changed parameter for the test run. An algorithm in <strong>bold</strong> indicates the best model in the algorithm comparison. A lower model loss and a higher accuracy is best.</p>
<h3 id="run-1">Run 1</h3>
<div>
<a id="tab:5.1"></a>
<table align="center">
<a id="tab:parameters_1"></a>
<thead>
<tr class="header">
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1<sup>st</sup> Layer</td>
<td style="text-align: left;">3x3 Convolution</td>
</tr>
<tr class="even">
<td style="text-align: left;">2<sup>nd</sup> Layer</td>
<td style="text-align: left;">2x2 Maxpooling</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Dropout</td>
<td style="text-align: left;">20%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Neurons in 1<sup>st</sup> FC Layer</td>
<td style="text-align: left;">512</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Neurons in 2<sup>nd</sup> FC Layer</td>
<td style="text-align: left;">6</td>
</tr>
<tr class="even">
<td style="text-align: left;">Metric</td>
<td style="text-align: left;">Categorical Cross-Entropy</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Epochs</td>
<td style="text-align: left;">50</td>
</tr>
</tbody>
</table>
<figure>
<figcaption text-align="center"><b class="captiontitle">Table 5.1: </b>Hyperparameter Table, Run 1.<span data-label="tab:parameters_1"></span></figcaption>
<figure>
<div>
<a id="tab:5.2"></a>
<p> </p>
<div>
<table align="center">
<a id="tab:results_1"></a>
<thead>
<tr class="header">
<th style="text-align: left;">Algorithm</th>
<th style="text-align: left;">Model Loss</th>
<th style="text-align: left;">Model Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Adadelta</td>
<td style="text-align: left;">1.3%</td>
<td style="text-align: left;">0.8%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adagrad</td>
<td style="text-align: left;">10.7%</td>
<td style="text-align: left;">0.3%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adam</td>
<td style="text-align: left;">1.3%</td>
<td style="text-align: left;">0.8%</td>
</tr>
<tr class="even">
<td style="text-align: left;">RMSProp</td>
<td style="text-align: left;">10.7%</td>
<td style="text-align: left;">0.3%</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>SGD</strong></td>
<td style="text-align: left;"><strong>0.8%</strong></td>
<td style="text-align: left;"><strong>0.8%</strong></td>
</tr>
</tbody>
</table>
<figcaption text-align="center"><b class="captiontitle">Table 5.2: </b>Results Table, Run 1.<span data-label="tab:results_1"></span></figcaption>
</figure>
</div>
<p> </p>
<figure>
<a id="fig:results_graphs_1"></a>
<img align="center" src="./figures/figure_31.svg" />
<p></p>
<figcaption style="text-align: center">&emsp;&emsp;Results Graphs, Run 1.<span data-label="fig:results_graphs_1"></span></figcaption>
</figure>
<h3 id="run-2">Run 2</h3>
<div>
<a id="tab:5.3"></a>
<table align="center">
<a id="tab:parameters_2"></a>
<thead>
<tr class="header">
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1<sup>st</sup> Layer</td>
<td style="text-align: left;">3x3 Convolution</td>
</tr>
<tr class="even">
<td style="text-align: left;">2<sup>nd</sup> Layer</td>
<td style="text-align: left;">2x2 Maxpooling</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Dropout</strong></td>
<td style="text-align: left;"><strong>50%</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">Neurons in 1<sup>st</sup> FC Layer</td>
<td style="text-align: left;">512</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Neurons in 2<sup>nd</sup> FC Layer</td>
<td style="text-align: left;">6</td>
</tr>
<tr class="even">
<td style="text-align: left;">Metric</td>
<td style="text-align: left;">Categorical Cross-Entropy</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Epochs</td>
<td style="text-align: left;">50</td>
</tr>
</tbody>
</table>
<figure>
<figcaption text-align="center"><b class="captiontitle">Table 5.3: </b>Hyperparameter Table, Run 2.<span data-label="tab:parameters_2"></span></figcaption>
</figure>
</div>
<p> </p>
<div>
<a id="tab:5.4"></a>
<table align="center">
<a id="tab:results_2"></a>
<thead>
<tr class="header">
<th style="text-align: left;">Algorithm</th>
<th style="text-align: left;">Model Loss</th>
<th style="text-align: left;">Model Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Adadelta</td>
<td style="text-align: left;">1.4%</td>
<td style="text-align: left;">0.7%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adagrad</td>
<td style="text-align: left;">1.0%</td>
<td style="text-align: left;">0.8%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adam</td>
<td style="text-align: left;">1.9%</td>
<td style="text-align: left;">0.7%</td>
</tr>
<tr class="even">
<td style="text-align: left;">RMSProp</td>
<td style="text-align: left;">1.5%</td>
<td style="text-align: left;">0.8%</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>SGD</strong></td>
<td style="text-align: left;"><strong>0.9%</strong></td>
<td style="text-align: left;"><strong>0.8%</strong></td>
</tr>
</tbody>
</table>
<figure>
<figcaption text-align="center"><b class="captiontitle">Table 5.4: </b>Results Table, Run 2.</figcaption>
</figure>
</div>
<p> </p>
<figure>
<a id="fig:results_graphs_2"></a>
<img align="center" src="./figures/figure_32.svg" />
<p> </p>
<figcaption style="text-align: center">&emsp;&emsp;Results Graphs, Run 2.<span data-label="fig:results_graphs_2"></span></figcaption>
</figure>
<h3 id="run-3">Run 3</h3>
<div>
<a id="tab:5.5"></a>
<table align="center">
<a id="tab:parameters_3"></a>
<thead>
<tr class="header">
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1<sup>st</sup> Layer</td>
<td style="text-align: left;">3x3 Convolution</td>
</tr>
<tr class="even">
<td style="text-align: left;">2<sup>nd</sup> Layer</td>
<td style="text-align: left;">2x2 Maxpooling</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>3<sup>rd</sup> Layer</strong></td>
<td style="text-align: left;"><strong>3x3 Convolution</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>2<sup>nd</sup> Layer</strong></td>
<td style="text-align: left;"><strong>2x2 Maxpooling</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Dropout</td>
<td style="text-align: left;">20%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Neurons in 1<sup>st</sup> FC Layer</td>
<td style="text-align: left;">512</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Neurons in 2<sup>nd</sup> FC Layer</td>
<td style="text-align: left;">6</td>
</tr>
<tr class="even">
<td style="text-align: left;">Metric</td>
<td style="text-align: left;">Categorical Cross-Entropy</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Epochs</td>
<td style="text-align: left;">50</td>
</tr>
</tbody>
</table>
<figure>
<figcaption><b class="captiontitle">Table 5.5: </b>Hyperparameter Table, Run 3.<span data-label="tab:parameters_3"></span></figcaption>
</figure>
</div>
<div>
<a id="tab:5.6"></a>
<table align="center">
<a id="tab:results_3"></a>
<thead>
<tr class="header">
<th style="text-align: left;">Algorithm</th>
<th style="text-align: left;">Model Loss</th>
<th style="text-align: left;">Model Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Adadelta</td>
<td style="text-align: left;">1.9%</td>
<td style="text-align: left;">0.8%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Adagrad</td>
<td style="text-align: left;">10.2%</td>
<td style="text-align: left;">0.4%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adam</td>
<td style="text-align: left;">1.1%</td>
<td style="text-align: left;">0.8%</td>
</tr>
<tr class="even">
<td style="text-align: left;">RMSProp</td>
<td style="text-align: left;">2.1%</td>
<td style="text-align: left;">0.8%</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>SGD</strong></td>
<td style="text-align: left;"><strong>1.0%</strong></td>
<td style="text-align: left;"><strong>0.8%</strong></td>
</tr>
</tbody>
<p> </p>
</table>
<figure>
<figcaption><b class="captiontitle">Table 5.6: </b>Results Table, Run 3.<span data-label="tab:results_3"></span></figcaption>
</figure>
</div>
<p> </p>
<figure>
<a id="fig:results_graphs_3"></a>
<img align="center" src="./figures/figure_33.svg" />
<p></p>
<figcaption style="text-align: center">&emsp;&emsp;Results Graphs, Run 3.<span data-label="fig:results_graphs_3"></span></figcaption>
</figure>
<h3 id="run-4">Run 4</h3>
<div>
<a id="tab:5.7"></a>
<table align="center">
<a id="tab:parameters_4"></a>
<thead>
<tr class="header">
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1<sup>st</sup> Layer</td>
<td style="text-align: left;">3x3 Convolution</td>
</tr>
<tr class="even">
<td style="text-align: left;">2<sup>nd</sup> Layer</td>
<td style="text-align: left;">2x2 Maxpooling</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3<sup>rd</sup> Layer</td>
<td style="text-align: left;">3x3 Convolution</td>
</tr>
<tr class="even">
<td style="text-align: left;">4<sup>th</sup> Layer</td>
<td style="text-align: left;">2x2 Maxpooling</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>5<sup>th</sup> Layer</strong></td>
<td style="text-align: left;"><strong>3x3 Convolution</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>6<sup>th</sup> Layer</strong></td>
<td style="text-align: left;"><strong>9x9 Maxpooling</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Dropout</strong></td>
<td style="text-align: left;"><strong>50%</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">Neurons in 1<sup>st</sup> FC Layer</td>
<td style="text-align: left;">512</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Neurons in 2<sup>nd</sup> FC Layer</td>
<td style="text-align: left;">6</td>
</tr>
<tr class="even">
<td style="text-align: left;">Metric</td>
<td style="text-align: left;">Categorical Cross-Entropy</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Epochs</td>
<td style="text-align: left;">50</td>
</tr>
</tbody>
</table>
<figure>
<figcaption><b class="captiontitle">Table 5.7: </b>Hyperparameter Table, Run 4.<span data-label="tab:parameters_4"></span></figcaption>
</figure>
<p> </p>
</div>
<div>
<a id="tab:5.8"></a>
<table align="center">
<a id="tab:results_4"></a>
<thead>
<tr class="header">
<th style="text-align: left;">Algorithm</th>
<th style="text-align: left;">Model Loss</th>
<th style="text-align: left;">Model Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Adadelta</strong></td>
<td style="text-align: left;"><strong>1.0%</strong></td>
<td style="text-align: left;"><strong>0.7%</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">Adagrad</td>
<td style="text-align: left;">1.1%</td>
<td style="text-align: left;">0.7%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adam</td>
<td style="text-align: left;">1.1%</td>
<td style="text-align: left;">0.7%</td>
</tr>
<tr class="even">
<td style="text-align: left;">RMSProp</td>
<td style="text-align: left;">1.2%</td>
<td style="text-align: left;">0.7%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SGD</td>
<td style="text-align: left;">1.1%</td>
<td style="text-align: left;">0.5%</td>
</tr>
</tbody>
</table>
<figure>
<figcaption><b class="captiontitle">Table 5.8: </b>Results Table, Run 4.<span data-label="tab:results_4"></span></figcaption>
</figure>
</div>
<p> </p>
<figure>
<a id="fig:results_graphs_4"></a>
<img align="center" src="./figures/figure_34.svg" />
<p></p>
<figcaption>&emsp;&emsp;Results Graphs, Run 4.<span data-label="fig:results_graphs_4"></span></figcaption>
</figure>
<h3 id="run-5">Run 5</h3>
<div>
<a id="tab:5.9"></a>
<table align="center">
<a id="tab:parameters_5"></a>
<thead>
<tr class="header">
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1<sup>st</sup> Layer</td>
<td style="text-align: left;">3x3 Convolution</td>
</tr>
<tr class="even">
<td style="text-align: left;">2<sup>nd</sup> Layer</td>
<td style="text-align: left;">2x2 Maxpooling</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Dropout</strong></td>
<td style="text-align: left;"><strong>50%</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">Neurons in 1<sup>st</sup> FC Layer</td>
<td style="text-align: left;">512</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Neurons in 2<sup>nd</sup> FC Layer</td>
<td style="text-align: left;">6</td>
</tr>
<tr class="even">
<td style="text-align: left;">Metric</td>
<td style="text-align: left;">Categorical Cross-Entropy</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Epochs</td>
<td style="text-align: left;">50</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Learning Rate</strong></td>
<td style="text-align: left;"><strong>0.01</strong></td>
</tr>
</tbody>
</table>
<figure>
<figcaption><b class="captiontitle">Table 5.9: </b>Hyperparameter Table, Run 5.<span data-label="tab:parameters_5"></span></figcaption>
</figure>
<p> </p>
</div>
<div>
<a id="tab:5.10"></a>
<table align="center">
<a id="tab:results_5"></a>
<thead>
<tr class="header">
<th style="text-align: left;">Algorithm</th>
<th style="text-align: left;">Model Loss</th>
<th style="text-align: left;">Model Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Adadelta</strong></td>
<td style="text-align: left;"><strong>0.6%</strong></td>
<td style="text-align: left;"><strong>0.8%</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">Adagrad</td>
<td style="text-align: left;">11.1%</td>
<td style="text-align: left;">0.3%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Adam</td>
<td style="text-align: left;">10.8%</td>
<td style="text-align: left;">0.3%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Rmsprop</td>
<td style="text-align: left;">2.2%</td>
<td style="text-align: left;">0.8%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">SGD</td>
<td style="text-align: left;">0.9%</td>
<td style="text-align: left;">0.7%</td>
</tr>
</tbody>
</table>
<figure>
<figcaption><b class="captiontitle">Table 5.10: </b>Results Table, Run 5.<span data-label="tab:results_5"></span></figcaption>
</figure>
</div>
<p> </p>
<figure>
<a id="fig:results_graphs_5"></a>
<img align="center" src="./figures/figure_35.svg" />
<p></p>
<figcaption>&emsp;&emsp;Results Graphs, Run 5.<span data-label="fig:results_graphs_5"></span></figcaption>

<figcaption>
<p><sup>1</sup>SGD learning rate (<span class="math inline">\(\eta\)</span>) was set to 0.01, with the momentum (<span class="math inline">\(\gamma\)</span>) set to 0.9 and using  (NAG) or <code>nesterov</code>.</p>
</figcaption>
</figure>
<p>  Having tested the algorithms for 5 test runs, the results of the best performing optimisation algorithms were collected and it was shown that <strong>SGD</strong> and <strong>Adadelta</strong> fit the model better throughout all of the 5 test runs for 50 epochs. Table <a href="#tab:5.11">5.11</a> shows the best results for all the test runs. Numerically, the best performing algorithm was <strong>Adadelta</strong> with a model loss of <strong><span class="math inline">\(\sim\)</span>60%</strong> and a model accuracy of <strong><span class="math inline">\(\sim\)</span>80%</strong>, <strong>SGD</strong> came in second with a model loss of <strong><span class="math inline">\(\sim\)</span>80%</strong> and a model accuracy of <strong><span class="math inline">\(\sim\)</span>80%</strong>. In the other test runs, RMSProp seemed to have overfitted the model with the highest model loss in all test runs.</p>
<p>Adagrad seemed to have performed the same as RMSProp. Adam sometimes overfits the model and underperforms in the last test run, but has performance comparable to Adadelta. SGD had consistent performance for 3 test runs which one could even argue that Adadelta could have had the best model by chance. But the results show that <strong>SGD</strong> and <strong>Adadelta</strong> seem to be the best choice for this dataset. It may be interesting to see what happens if the epochs are increased for both <strong>SGD</strong> and <strong>Adadelta</strong> to find out if that separates the two and still fits the model. But due to time constraints and only one GPU, the epochs are reduced.</p>
<p>While <strong>Adadelta</strong> is the best according to the results, <strong>SGD</strong> seems to be <em>better</em> due to it’s consistent accuracy for three test runs, despite it’s first test model loss being 20% lower than <strong>Adadelta</strong>. Emotion classification results using different algorithms are shown in Appendix <a href="#sec:emotion-classification-results">D</p>.
<p>      </p>
<div>
<a id="tab:5.11"></a>
<table align="center">
<thead>
<tr class="header">
<th style="text-align: left;">Run</th>
<th style="text-align: left;">Algorithm</th>
<th style="text-align: left;">Model Loss</th>
<th style="text-align: left;">Model Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1</td>
<td style="text-align: left;">SGD</td>
<td style="text-align: left;">0.8%</td>
<td style="text-align: left;">0.8%</td>
</tr>
<tr class="even">
<td style="text-align: left;">2</td>
<td style="text-align: left;">SGD</td>
<td style="text-align: left;">0.9%</td>
<td style="text-align: left;">0.8%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3</td>
<td style="text-align: left;">SGD</td>
<td style="text-align: left;">1.0%</td>
<td style="text-align: left;">0.8%</td>
</tr>
<tr class="even">
<td style="text-align: left;">4</td>
<td style="text-align: left;">Adadelta</td>
<td style="text-align: left;">1.0%</td>
<td style="text-align: left;">0.7%</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>5</strong></td>
<td style="text-align: left;"><strong>Adadelta</strong></td>
<td style="text-align: left;"><strong>0.6%</strong></td>
<td style="text-align: left;"><strong>0.8%</strong></td>
</tr>
</tbody>
</table>
<figure>
<figcaption><b class="captiontitle">Table 5.11: </b>Algorithms with the best loss accuracy out of all 5 runs.<span data-label="tab:best_of_5_results"></span></figcaption>
</figure>
</div>
<h1 id="sec:chap:reflection">Reflection</h1>
<p>In retrospect, the results of training the dataset could be further improved if more emotions were added. Currently there exists only the three emotions that have been analysed in this report. The network was shallower than I had expected and had lots of overfitting issues when testing with more layers of convolutions and maxpooling, in Table <a href="#tab:5.5">5.5</a> and <a href="#tab:5.7">5.7</a> adding more layers to the network was overfitting the dataset. Increasing the dropout helped slightly in <a href="#tab:5.7">5.7</a> but I think that it would perform better if more emotions and data was added to the dataset, which only then this would constitute looking at a more deeper architecture. <span class="citation" data-cites="Goodfellow-et-al-2016">Goodfellow, Bengio, and Courville (<a href="#ref-Goodfellow-et-al-2016">2016</a>)</span> explains that deeper networks use few parameters per layer and are able to generalise the test set, but are harder to optimise <span class="citation" data-cites="Goodfellow-et-al-2016">(<a href="#ref-Goodfellow-et-al-2016">2016</a>, 192)</span>. In addition, If more time was possible, adding more training/testing data than the 400 would be one of the things I would do differently.</p>
<p>  I didn’t consider using a pre-trained model such as VGG16 or Resnet and fine tuning it; although I was aware of such models and methods. In the future, this would be an ideal starting point to compare the pre-trained models against this model that was trained from scratch.</p>
<p>  Looking back at the original risk matrix, the first risk was mitigated, effective and led to the success of the project. The second risk was not an issue since only 64 videos were used and I only focused on the main characters since they appeared more than the rest of them. This project would not be feasible for more than two or three characters because a lot of time would need to be invested to segment the faces. For N &lt;  3 characters, segmenting N characters may not scale in practice and will take too much time.</p>
<p>  The third risk was the second highest in impact because the data preparation was the longest in practice. It was unlikely to find an existing one online. But constructing the dataset was a great experience and may even be used in other applications if open sourced online. The fourth risk was never met since the dataset was small in size to begin with; in terms of the 400 test/training emotions. Finally, the last risk did not happen. Yet, the artefact does misclassify some images but not to warrant an entirely new dataset.</p>
<p> </p>
<div>
<a id="tab:6.1"></a>
<table align="center">
    <tr>
        <th style="text-align: left;">Risk</th>
        <th style="text-align: left;">Likelihood</th>
        <th style="text-align: left;">Impact</th>
        <th style="text-align: left;">Risk Quotient</th>
        <th style="text-align: left;">Contingency</th>
    </tr>
    <tr>
        <td style="text-align: left;">Large number of data,<br>many parameters.</td>
        <td style="text-align: center;">5</td>
        <td style="text-align: center;">4</td>
        <td style="text-align: center;">20</td>
        <td style="text-align: left;">Clean dataset and reduce<br>to only 3 emotions.</td>
    </tr>
    <tr>
        <td style="text-align: left;">Analysing the set of all cartoons<br>for emotions.</td>
        <td style="text-align: center;">2</td>
        <td style="text-align: center;">0.6</td>
        <td style="text-align: center;">1.2</td>
        <td style="text-align: left;">Only choose cartoons/animations<br>with only high<br>emotion.</td>
    </tr>
    <tr>
        <td style="text-align: left;">Cannot find relevant<br>dataset.</td>
        <td style="text-align: center;">4</td>
        <td style="text-align: center;">9</td>
        <td style="text-align: center;">36</td>
        <td style="text-align: left;">Research or filter ImageNet<br>dataset or construct a new<br>dataset by hand ahead of<br>training and design.</td>
    </tr>
    <tr>
        <td style="text-align: left;">Bad parameters.</td>
        <td style="text-align: center;">6</td>
        <td style="text-align: center;">7</td>
        <td style="text-align: center;">3</td>
        <td style="text-align: left;">Reduce the dataset if it is<br>too large.</td>
    </tr>
    <tr>
        <td style="text-align: left;">Cannot detect emotion in<br>character.</td>
        <td style="text-align: center;">5</td>
        <td style="text-align: center;">10</td>
        <td style="text-align: center;">50</td>
        <td style="text-align: left;">Quickly find another cartoon<br>dataset or decide on<br>another cartoon to analyse.</td>
    </tr>
</table>
<figure>
<figcaption><b class="captiontitle">Table 6.1: </b>The original risk matrix from project proposal<span data-label="tab:risk_matrix"></span></figcaption>
</figure>
</div>

<p>Nevertheless, I think this project does show promise of the possibility of combining deep learning, emotion recognition and cartoons. Animators that work in the 2D cartoons domain, illustrators and cartoonists would benefit from this work or variants of it. I think this project is a step forward, but there exists some room for further improvements.</p>
<h1 id="sec:cartoon-face-segmentation-1">Appendix A - Cartoon Face Segmentation</h1>
<p>The following code segments a face from a cartoon for dataset preparation.</p>

<figure>
<h3>
<a href="https://github.com/hako/dissertation/blob/master/segmentation.py"><code>segmentation.py</code></a>
</h3>
<figcaption><b class="captiontitle">Listing 5: </b>The source code for the cartoon face segmentation tool. It reads a series of videos and
saves a detected face by using a custom Haar cascade. Only the two characters Tom & Jerry have
separate Haar cascades.<span data-label="sec:cartoon-face-segmentation-1"></span></figcaption>
</figure>
    
<h1 id="sec:list-of-episodes-in-dataset">Appendix B - List of Episodes in Dataset</h1>
<p>Below is a list of <em>Tom &amp; Jerry</em> episodes that are used and included in the dataset, and have been created from scratch.</p>
<div>
<a id="tab:dataset_videos_1"></a>
<table align="center">
    <tr>
        <th>Episode</th>
        <th>Duration</th>
    </tr>
    <tr>
        <td><i>The Million Dollar Cat (1944)</i></td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>The Bodyguard (1944)</i></td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Puttin’ on the Dog (1944)</i></td>
        <td>2:09</td>
    </tr>
    <tr>
        <td><i>The Mouse Comes to Dinner (1945)</i></td>
        <td>2:16</td>
    </tr>
    <tr>
        <td><i>Mouse in Manhattan (1945)</i></td>
        <td>2:35</td>
    </tr>
    <tr>
        <td><i>Tee for Two (1945)</i></td>
        <td>2:14</td>
    </tr>
    <tr>
        <td><i>Flirty Birdy (1945)</i></td>
        <td>2:10</td>
    </tr>
    <tr>
        <td><i>Quiet Please! (1945)</i></td>
        <td>2:21</td>
    </tr>
    <tr>
        <td><i>Springtime for Thomas (1946)</i></td>
        <td>2:43</td>
    </tr>
    <tr>
        <td><i>The Milky Waif (1946)</i></td>
        <td>2:21</td>
    </tr>
    <tr>
        <td>Trap Happy (1946)</i></td>
        <td>2:10</td>
    </tr>
    <tr>
        <td><i>Solid Serenade (1946)</i></td>
        <td>2:38</td>
    </tr>
    <tr>
        <td><i>Cat Fishin' (1947)</i></td>
        <td>2:17</td>
    </tr>
    <tr>
        <td><i>Part Time Pal (1947)</i></td>
        <td>2:53</td>
    </tr>
    <tr>
        <td><i>The Cat Concerto (1947)</i></td>
        <td>1:44</td>
    </tr>
    <tr>
        <td><i>Dr. Jekyll and Mr. Mouse (1947)</i></td>
        <td>2:25</td>
    </tr>
    <tr>
        <td><i>Salt Water Tabby (1947)</i></td>
        <td>2:15</td>
    </tr>
    <tr>
        <td><i>A Mouse in the House (1947)</i></td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>The Invisible Mouse (1947)</i></td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Kitty Foiled (1948)</i></td>
        <td>2:57</td>
    </tr>
    <tr>
        <td><i>The Truce Hurts (1948)</i></td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Old Rockin' Chair Tom (1948)</i></td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Professor Tom (1948)</i></td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Mouse Cleaning (1948)</i></td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Polka Dot Puss (1949)</i></td>
        <td>2:56</td>
    </tr>
    <tr>
        <td><i>The Little Orphan (1949)</i></td>
        <td>2:55</td>
    </tr>
    <tr>
        <td><i>Hatch Up Your Troubles (1949)</i></td>
        <td>2:58</td>
    </tr>
    <tr>
        <td><i>Heavenly Puss (1949)</i></td>
        <td>2:59</td>
    </tr>
    <tr>
        <td><i>The Cat and the Mermouse (1949)</i></td>
        <td>2:59</td>
    </tr>
    <tr>
        <td><i>Love That Pup (1949)</i></td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Jerry's Diary (1949)</i></td>
        <td>3:00</td>
    </tr>
</table>
<p></p>
<figcaption align="center">The first 32 <em>Tom &amp; Jerry</em> episodes of the images that are included in the dataset of this project.<span data-label="tab:dataset_videos_1"></span></figcaption>
</div>

<div>
<a id="tab:dataset_videos_2"></a>
<table align="center">
    <tr>
        <th>Episode</th>
        <th>Duration</th>
    </tr>
    <tr>
        <td><i>Tennis Chumps (1949)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Little Quacker (1950)</td>
        <td>2:59</td>
    </tr>
    <tr>
        <td><i>Saturday Evening Puss (1950)</td>
        <td>2:57</td>
    </tr>
    <tr>
        <td><i>Texas Tom (1950)</td>
        <td>2:58</td>
    </tr>
    <tr>
        <td><i>Jerry and the Lion (1950)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Safety Second (1950)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>The Hollywood Bowl (1950)</td>
        <td>2:59</td>
    </tr>
    <tr>
        <td><i>The Framed Cat (1950)</td>
        <td>2:59</td>
    </tr>
    <tr>
        <td><i>Cue Ball Cat (1950)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Casanova Cat (1951)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Jerry and the Goldfish (1951)</td>
        <td>2:59</td>
    </tr>
    <tr>
        <td><i>Jerry's Cousin (1951)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Sleepy Time Tom (1951)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>His Mouse Friday (1951)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Slicked up Pup (1951)</td>
        <td>2:59</td>
    </tr>
    <tr>
        <td><i>Nit Witty Kitty (1951)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Cat Napping (1951)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>The Flying Cat (1952)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>The Duck Doctor (1952)</td>
        <td>2:59</td>
    </tr>
    <tr>
        <td><i>The Two Mouseketeers (1952)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Triplet Trouble (1952)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Fit to be Tied (1952)</td>
        <td>2:58</td>
    </tr>
    <tr>
        <td><i>The Missing Mouse (1953)</td>
        <td>2:58</td>
    </tr>
    <tr>
        <td><i>Jerry and Jumbo (1953)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Johann Mouse (1953)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Two Little Indians (1953)</td>
        <td>2:59</td>
    </tr>
    <tr>
        <td><i>Life with Tom (1953)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Hic cup Pup (1954)</td>
        <td>2:58</td>
    </tr>
    <tr>
        <td><i>Baby Butch (1954)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Mice Follies (1954)</td>
        <td>3:00</td>
    </tr>
    <tr>
        <td><i>Neapolitan Mouse (1954)</td>
        <td>2:59</td>
    </tr>
    <tr>
        <td><i>Pet Peeve (1954)</td>
        <td>2:59</td>
    </tr>
</table>
<h1 id="sec:classification">Appendix C - Classification</h1>
<p>The following code below classifies a randomly selected face of any of the 3 emotions from the test set. By default, <code>-t</code> trains the model and <code>-t -V</code> trains and classifies the test dataset. Other operations include creating visualisations of convolution layers and the output layer.<br />
</p>
<figure>
<h3>
<a href="https://github.com/hako/dissertation/blob/master/train.py"><code>train.py</code></a>
</h3>
<figcaption><b class="captiontitle">Listing 6: </b>The source code for the training, classification and visualisation tool.<span data-label=""></span></figcaption>
</figure>
<h1 id="sec:emotion-classification-results">Appendix D - Emotion Classification Results</h1>
<p>The following images are the results of emotion classification of the test set that are classified by the artefact. The best models from 1-5 runs of the algorithm including two of the best models in Table <a href="#tab:5.11">5.11</a> are shown below. Each image shows a prediction, green indicates a correct prediction. Red indicates misclassification with the correct label (also in green).<br />
</p>
<div align="center">
<p><span></span> <img align="center" width="75%" src="./figures/figure_36.svg" title="Tested on best model for Adadelta." /></p>
<figcaption align="center"><b class="captiontitle">(a) </b>Tested on best model for Adadelta, in Table <a href="#tab:5.9">5.9</a><span data-label="fig:emotion_classification_adadelta"></span></figcaption>

<p><span></span> <img align="center" width="75%" src="./figures/figure_40.svg" title="Tested on best model for SGD." /></p>
<figcaption align="center"><b class="captiontitle">(b) </b>Tested on best model for SGD, in Table <a href="#tab:5.1">5.1</a><span data-label="fig:emotion_classification_sgd"></span></figcaption>

<p><span></span> <img align="center" width="75%" src="./figures/figure_38.svg" title="Tested on best model for Adam." /></p>
<figcaption align="center"><b class="captiontitle">(c) </b>Tested on best model for Adam, in Table <a href="#tab:5.5">5.5</a><span data-label="fig:emotion_classification_adam"></span></figcaption>

<p><span></span> <img align="center" width="75%" src="./figures/figure_37.svg" title="Tested on best model for Adagrad." /></p>
<figcaption align="center"><b class="captiontitle">(c) </b>Tested on best model for Adagrad, in Table <a href="#tab:5.3">5.3</a><span data-label="fig:emotion_classification_adagrad"></span></figcaption>

<p><span></span> <img align="center" width="75%" src="./figures/figure_39.svg" title="Tested on best model for RMSProp." /></p>
<figcaption align="center"><b class="captiontitle">(c) </b>Tested on best model for RMSProp, in Table <a href="#tab:5.7">5.7</a><span data-label="fig:emotion_classification_rmsprop"></span></figcaption>
</div>

<h1 id="sec:references" class="references">References</h1>
<div id="ref-Abadi:2016vn">
<p>Abadi, M., Barham P., Chen J., Chen Z., Davis A., Dean J., Devin M., et al. 2016. “TensorFlow - A system for large-scale machine learning.” <em>CoRR</em> cs.DC.</p>
</div>
<div id="ref-belortaja:2009">
<p>Belortaja, M. 2009. “New Best Friends.” <a href="https://www.toonpool.com/cartoons/new%20best%20friends_67370">https://www.toonpool.com/cartoons/new%20best%20friends_67370</a>.</p>
</div>
<div id="ref-Bishop:1995:NNP:525960">
<p>Bishop, C.M. 1995. <em>Neural Networks for Pattern Recognition</em>. New York, NY, USA: Oxford University Press, Inc.</p>
</div>
<div id="ref-britton:2017">
<p>Britton, M. 2017. “Chair Drawing.” <a href="https://www.shutterstock.com/image-vector/chair-drawing-59225347" class="uri">https://www.shutterstock.com/image-vector/chair-drawing-59225347</a>.</p>
</div>
<div id="ref-ERIHCI:911197">
<p>Cowie, R., E. Douglas, N. Tsapatsoulis, G. Votsis, S. Kollias, W. Fellenz, and J.G. Taylor. 2001. “Emotion Recognition in Human-Computer Interaction.” <em>IEEE Signal Processing Magazine</em> 18 (1): 32–80. doi:<a href="https://doi.org/10.1109/79.911197">10.1109/79.911197</a>.</p>
</div>
<div id="ref-darwin1872expression">
<p>Darwin, C. 1872. <em>The Expression of the Emotions in Man and Animals</em>. Classics of Psychiatry &amp; Behavioral Sciences Library. Impression anastalitique Culture et Civilisation.</p>
</div>
<div id="ref-deJuan:2004ex">
<p>de Juan, C., and B. Bodenheimer. 2004. “Cartoon textures.” <em>Symposium on Computer Animation</em>.</p>
</div>
<div id="ref-descartes_1985">
<p>Descartes, R. 1985. <em>The Philosophical Writings of Descartes</em>. Vol. 1. Cambridge University Press. doi:<a href="https://doi.org/10.1017/CBO9780511805042">10.1017/CBO9780511805042</a>.</p>
</div>
<div id="ref-ekman1971constants">
<p>Ekman, P., and W.V. Friesen. 1971. “Constants Across Cultures in the Face and Emotion.” <em>Journal of Personality and Social Psychology</em> 17 (2). American Psychological Association: 124.</p>
</div>
<div id="ref-Ertel:2011:IAI:1971988">
<p>Ertel, W. 2011. <em>Introduction to Artificial Intelligence</em>. 1<sup>st</sup> ed. Springer Publishing Company, Incorporated.</p>
</div>
<div id="ref-fukushima1980neocognitron">
<p>Fukushima, K. 1980. “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position.” <em>Biological Cybernetics</em> 36 (4). Springer: 193–202.</p>
</div>
<div id="ref-Gajarla:us">
<p>Gajarla, V., and A. Gupta. n.d. “Emotion Detection and Sentiment Analysis of Images.” <em>Cc.gatech.edu</em>.</p>
</div>
<div id="ref-Glorot:2011tm">
<p>Glorot, X., A. Bordes, and Y. Bengio. 2011. “Deep Sparse Rectifier Neural Networks.” <em>AISTATS</em>.</p>
</div>
<div id="ref-Goodfellow-et-al-2016">
<p>Goodfellow, I., Y. Bengio, and A. Courville. 2016. <em>Deep Learning</em>. MIT Press.</p>
</div>
<div id="ref-DBLP:journals/corr/GravesWD14">
<p>Graves, A., G. Wayne, and I. Danihelka. 2014. “Neural Turing Machines.” <em>CoRR</em> abs/1410.5401. <a href="http://arxiv.org/abs/1410.5401" class="uri">http://arxiv.org/abs/1410.5401</a>.</p>
</div>
<div id="ref-hebb:1949">
<p>Hebb, D.O. 1949. <em>The Organization of Behavior: A Neuropsychological Approach</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-hess2009darwin">
<p>Hess, U., and P. Thibault. 2009. “Darwin and Emotion Expression.” <em>American Psychologist</em> 64 (2). American Psychological Association: 120.</p>
</div>
<div id="ref-HinSal06">
<p>Hinton, G.E., and R. Salakhutdinov. 2006. “Reducing the Dimensionality of Data with Neural Networks.” <em>Science</em> 313 (5786): 504–7.</p>
</div>
<div id="ref-Hochreiter:1997:LSM:1246443.1246450">
<p>Hochreiter, S., and J. Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Computation</em> 9 (8). Cambridge, MA, USA: MIT Press: 1735–80. doi:<a href="https://doi.org/10.1162/neco.1997.9.8.1735">10.1162/neco.1997.9.8.1735</a>.</p>
</div>
<div id="ref-Hu:2015uo">
<p>Hu, B., Z. Lu, H. Li, and Q. Chen. 2015. “Convolutional Neural Network Architectures for Matching Natural Language Sentences.” <em>arXiv.org</em>, March. <a href="http://arxiv.org/abs/1503.03244v1" class="uri">http://arxiv.org/abs/1503.03244v1</a>.</p>
</div>
<div id="ref-hubel1962receptive">
<p>Hubel, D.H., and T.N. Wiesel. 1962. “Receptive Fields, Binocular Interaction and Functional Architecture in the Cat’s Visual Cortex.” <em>The Journal of Physiology</em> 160 (1). Wiley Online Library: 106–54.</p>
</div>
<div id="ref-Jalled:2016vi">
<p>Jalled, F., and I. Voronkov. 2016. “Object Detection using Image Processing.” <em>arXiv.org</em>, November. <a href="http://arxiv.org/abs/1611.07791v1" class="uri">http://arxiv.org/abs/1611.07791v1</a>.</p>
</div>
<div id="ref-Kahou:2015cr">
<p>Kahou, S.E., Michalski V., Konda K.R., Memisevic R., and C.J. Pal. 2015. “Recurrent Neural Networks for Emotion Recognition in Video.” <em>ICMI</em>, 467–74.</p>
</div>
<div id="ref-Kingma:2014us">
<p>Kingma, D.P, and J. Ba. 2014. “Adam: A Method for Stochastic Optimization.” <em>arXiv.org</em>, December. <a href="http://arxiv.org/abs/1412.6980v9" class="uri">http://arxiv.org/abs/1412.6980v9</a>.</p>
</div>
<div id="ref-Krizhevsky2012">
<p>Krizhevsky, A., I. Sutskever, and G.E. Hinton. 2012. “ImageNet Classification with Deep Convolutional Neural Networks.” In <em>Advances in Neural Information Processing Systems 25</em>, edited by F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, 1097–1105. Curran Associates, Inc. <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networkspng" class="uri">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networkspng</a>.</p>
</div>
<div id="ref-Le:2015vt">
<p>Le, Q.V., N. Jaitly, and G.E. Hinton. 2015. “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units.” <em>arXiv.org</em>, April. <a href="http://arxiv.org/abs/1504.00941v2" class="uri">http://arxiv.org/abs/1504.00941v2</a>.</p>
</div>
<div id="ref-LeCun:NIPS1989_293">
<p>LeCun, Y., B.E. Boser, J.S. Denker, D. Henderson, R.E. Howard, W.E. Hubbard, and L.D. Jackel. 1990. “Handwritten Digit Recognition with a Back-Propagation Network.” In <em>Advances in Neural Information Processing Systems 2</em>, edited by D. S. Touretzky, 396–404. Morgan-Kaufmann. <a href="http://papers.nips.cc/paper/293-handwritten-digit-recognition-with-a-back-propagation-networkpng" class="uri">http://papers.nips.cc/paper/293-handwritten-digit-recognition-with-a-back-propagation-networkpng</a>.</p>
</div>
<div id="ref-LeCun:98">
<p>LeCun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” <em>Proceedings of the IEEE</em> 86 (11): 2278–2324. doi:<a href="https://doi.org/10.1109/5.726791">10.1109/5.726791</a>.</p>
</div>
<div id="ref-LeCun:2015dt">
<p>LeCun, Y., Bengio Y., and G. Hinton. 2015. “Deep learning.” <em>Nature</em> 521 (7553): 436–44.</p>
</div>
<div id="ref-Lipton:2015tj">
<p>Lipton, Z.C., J. Berkowitz, and C. Elkan. 2015. “A Critical Review of Recurrent Neural Networks for Sequence Learning.” <em>arXiv.org</em>, May. <a href="http://arxiv.org/abs/1506.00019v4" class="uri">http://arxiv.org/abs/1506.00019v4</a>.</p>
</div>
<div id="ref-Lisetti2003245">
<p>Lisetti, C., F. Nasoz, C. LeRouge, O. Ozyer, and K. Alvarez. 2003. “Developing Multimodal Intelligent Affective Interfaces for Tele-Home Health Care.” <em>International Journal of Human-Computer Studies</em> 59 (1). Elsevier: 245–55.</p>
</div>
<div id="ref-Lowel209">
<p>Lowel, S., and W. Singer. 1992. “Selection of Intrinsic Horizontal Connections in the Visual Cortex by Correlated Neuronal Activity.” <em>Science</em> 255 (5041). American Association for the Advancement of Science: 209–12. <a href="http://science.sciencemag.org/content/255/5041/209" class="uri">http://science.sciencemag.org/content/255/5041/209</a>.</p>
</div>
<div id="ref-Marsland:2014:MLA:2692349">
<p>Marsland, S. 2014. <em>Machine Learning: An Algorithmic Perspective</em>. 2<sup>nd</sup> ed. Chapman &amp; Hall/CRC.</p>
</div>
<div id="ref-minsky69perceptrons">
<p>Minsky, M., and S. Papert. 1969. <em>Perceptrons: An Introduction to Computational Geometry</em>. Cambridge, MA, USA: MIT Press.</p>
</div>
<div id="ref-Nakazawa:2016ip">
<p>Nakazawa, S., and T. Tanaka. 2016. “Development and Application of Kanban Tool Visualizing the Work in Progress.” In <em>2016 5<sup>th</sup> Iiai International Congress on Advanced Applied Informatics (Iiai-Aai)</em>, 908–13. IEEE.</p>
</div>
<div id="ref-Patterson:1998:ANN:521611">
<p>Patterson, D.W. 1998. <em>Artificial Neural Networks: Theory and Applications</em>. 1<sup>st</sup> ed. Upper Saddle River, NJ, USA: Prentice Hall PTR.</p>
</div>
<div id="ref-Perez:2011tp">
<p>Pérez, F., B.E Granger, and J.D Hunter. 2011. “Python - An Ecosystem for Scientific Computing.” <em>Computing in Science and Engineering</em>.</p>
</div>
<div id="ref-picard1997affective">
<p>Picard, R.W. 2000. <em>Affective Computing</em>. MIT Press.</p>
</div>
<div id="ref-Rajanna:2016ux">
<p>Rajanna, A.R., K. Aryafar, R. Ramchandran, C. Sisson, A. Shokoufandeh, and R. Ptucha. 2016. “Neural Networks with Manifold Learning for Diabetic Retinopathy Detection.” <em>CoRR</em> cs.CV.</p>
</div>
<div id="ref-randomlists.com">
<p>Randomlists.com. n.d. “Zebra.” <a href="https://www.randomlists.com/img/animals/zebra.jpg" class="uri">https://www.randomlists.com/img/animals/zebra.jpg</a>.</p>
</div>
<div id="ref-rayner">
<p>Rayner, S. n.d. “How to Draw a Deck Chair Real Easy.” <a href="http://www.shoorayner.com/how-to-draw-a-deck-chair-real-easy/" class="uri">http://www.shoorayner.com/how-to-draw-a-deck-chair-real-easy/</a>.</p>
</div>
<div id="ref-Roberts:2017:Online">
<p>Roberts, E. 2006. “Neural Networks - Architecture.” <em>Cs.stanford.edu</em>. December. <a href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/feedforward.html" class="uri">https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/feedforward.html</a>.</p>
</div>
<div id="ref-Roberts:2012ww">
<p>Roberts, K., M.A. Roach, J. Johnson, J. Guthrie, and S.M. Harabagiu. 2012. “EmpaTweet - Annotating and Detecting Emotions on Twitter.” <em>LREC</em>.</p>
</div>
<div id="ref-Rojas:1996:NNS:235222">
<p>Rojas, R. 1996. <em>Neural Networks: A Systematic Introduction</em>. New York, NY, USA: Springer-Verlag New York, Inc.</p>
</div>
<div id="ref-Ruder:2016tr">
<p>Ruder, S. 2016. “An overview of gradient descent optimization algorithms.” <em>CoRR</em>.</p>
</div>
<div id="ref-Rumelhart:1986:LIR:104279.104293">
<p>Rumelhart, D.E., G.E. Hinton, and R.J. Williams. 1986. “Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1.” In, 318–62. Cambridge, MA, USA: MIT Press.</p>
</div>
<div id="ref-Russakovsky:2014vi">
<p>Russakovsky, O., J. Deng, H Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, et al. 2014. “ImageNet Large Scale Visual Recognition Challenge.” <em>arXiv.org</em>, September. <a href="http://arxiv.org/abs/1409.0575v3" class="uri">http://arxiv.org/abs/1409.0575v3</a>.</p>
</div>
<div id="ref-schatz1992developing">
<p>Schatz, C.J. 1992. “The Developing Brain.” <em>Scientific American</em> 267 (3). ERIC: 60–67.</p>
</div>
<div id="ref-Schmidhuber:2014cz">
<p>Schmidhuber, J. 2014. “Deep Learning in Neural Networks: An Overview.” <em>arXiv.org</em>, April, 85–117. <a href="http://arxiv.org/abs/1404.7828v4" class="uri">http://arxiv.org/abs/1404.7828v4</a>.</p>
</div>
<div id="ref-Shaydulin:2017ty">
<p>Shaydulin, R., and J. Sybrandt. 2017. “To Agile, or not to Agile - A Comparison of Software Development Methodologies.” <em>CoRR</em> cs.SE.</p>
</div>
<div id="ref-shiplett:2013">
<p>Shiplett, P. 2013. “The Way Too Easy Chair.” <a href="http://shiplett.com/wp-content/uploads/2015/12/Easy-Chair6-550px.jpg" class="uri">http://shiplett.com/wp-content/uploads/2015/12/Easy-Chair6-550px.jpg</a>.</p>
</div>
<div id="ref-Strichartz:2003tk">
<p>Strichartz, R.S. 2003. <em>A Guide to Distribution Theory and Fourier Transforms</em>. World Scientific.</p>
</div>
<div id="ref-Talathi:2015uv">
<p>Talathi, S.S, and A. Vartak. 2015. “Improving performance of recurrent neural network with relu nonlinearity.” <em>arXiv.org</em>, November. <a href="http://arxiv.org/abs/1511.03771v3" class="uri">http://arxiv.org/abs/1511.03771v3</a>.</p>
</div>
<div id="ref-openclipart_vectors:2013">
<p>Vectors, OpenClipart. 2013. “Sofa.” <a href="https://pixabay.com/en/settee-sofa-couch-furniture-147701/" class="uri">https://pixabay.com/en/settee-sofa-couch-furniture-147701/</a>.</p>
</div>
<div id="ref-Viola:2001ks">
<p>Viola, P., and M Jones. 2001. “Rapid object detection using a boosted cascade of simple features.” In <em>2001 Ieee Computer Society Conference on Computer Vision and Pattern Recognition. Cvpr 2001</em>, I–511–I–518. IEEE Comput. Soc.</p>
</div>
<div id="ref-wang:2017">
<p>Wang, H., B. Raj, and E.P. Xing. 2017. “On the Origin of Deep Learning.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1702.07800v4" class="uri">http://arxiv.org/abs/1702.07800v4</a>.</p>
</div>
<div id="ref-warner_bros:2013">
<p>Warner Bros. 2013. “Bugs Bunny.” <a href="http://thevoiceofyourbusiness.com/blog/wp-content/uploads/2014/02/bugs-bunnyreclining-499x367-300x220.jpg" class="uri">http://thevoiceofyourbusiness.com/blog/wp-content/uploads/2014/02/bugs-bunnyreclining-499x367-300x220.jpg</a>.</p>
</div>
<div id="ref-yu2013modern">
<p>Yu, J., and D. Tao. 2013. <em>Modern Machine Learning Techniques and Their Applications in Cartoon Animation Research</em>. Vol. 4. John Wiley &amp; Sons.</p>
</div>
<div id="ref-Zafeiriou:2016kn">
<p>Zafeiriou, S, A. Papaioannou, I. Kotsia, M Nicolaou, and G. Zhao. 2016. “Facial Affect In-the-Wild: A Survey and a New Database.” In <em>2016 Ieee Conference on Computer Vision and Pattern Recognition Workshops (Cvprw</em>, 1487–98. IEEE.</p>
</div>
<div id="ref-Zeiler:2012uw">
<p>Zeiler, M.D. 2012. “ADADELTA: An Adaptive Learning Rate Method.” <em>arXiv.org</em>, December. <a href="http://arxiv.org/abs/1212.5701v1" class="uri">http://arxiv.org/abs/1212.5701v1</a>.</p>
</div>
</div>
</body>
</html>
